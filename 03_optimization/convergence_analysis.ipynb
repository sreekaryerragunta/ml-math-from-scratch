{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Convergence Analysis\n",
                "\n",
                "Deep dive into convergence behavior:\n",
                "1. Learning rate effects\n",
                "2. Convergence criteria\n",
                "3. Learning rate schedules\n",
                "4. Momentum\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "np.random.seed(42)\n",
                "\n",
                "# Setup data\n",
                "m = 100\n",
                "X = 2 * np.random.rand(m, 1)\n",
                "y = 5 + 3 * X + np.random.randn(m, 1)\n",
                "X_b = np.c_[np.ones((m, 1)), X]\n",
                "\n",
                "def compute_cost(X, y, theta):\n",
                "    m = len(y)\n",
                "    return (1/(2*m)) * np.sum((X.dot(theta) - y)**2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Analysis 1: Learning Rate Impact"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gd_with_lr(X, y, lr, n_iter=100):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    costs = []\n",
                "    \n",
                "    for _ in range(n_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        theta = theta - lr * gradients\n",
                "        costs.append(compute_cost(X, y, theta))\n",
                "    \n",
                "    return theta, costs\n",
                "\n",
                "# Test different learning rates\n",
                "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
                "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
                "\n",
                "plt.figure(figsize=(12, 7))\n",
                "for lr, color in zip(learning_rates, colors):\n",
                "    _, costs = gd_with_lr(X_b, y, lr, n_iter=100)\n",
                "    plt.plot(costs, linewidth=2, color=color, label=f'α = {lr}', alpha=0.8)\n",
                "\n",
                "plt.xlabel('Iteration', fontsize=13)\n",
                "plt.ylabel('Cost', fontsize=13)\n",
                "plt.title('Effect of Learning Rate on Convergence', fontsize=15, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.yscale('log')  # Log scale to see all curves\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Observation\n",
                "\n",
                "- **α = 0.001**: Too small, very slow\n",
                "- **α = 0.01**: Slow but steady\n",
                "- **α = 0.1**: Good convergence\n",
                "- **α = 0.5**: Fast but may oscillate\n",
                "- **α = 1.0**: Too large, diverges!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Analysis 2: Convergence Criteria"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gd_with_criteria(X, y, lr=0.1, tolerance=1e-6, max_iter=1000):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    costs = [compute_cost(X, y, theta)]\n",
                "    \n",
                "    for iteration in range(max_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        theta = theta - lr * gradients\n",
                "        cost = compute_cost(X, y, theta)\n",
                "        costs.append(cost)\n",
                "        \n",
                "        # Check convergence\n",
                "        if abs(costs[-1] - costs[-2]) < tolerance:\n",
                "            print(f'Converged at iteration {iteration}')\n",
                "            break\n",
                "    \n",
                "    return theta, costs\n",
                "\n",
                "theta_conv, costs_conv = gd_with_criteria(X_b, y, tolerance=1e-6)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(costs_conv, linewidth=2, color='darkblue')\n",
                "plt.axhline(y=costs_conv[-1], color='red', linestyle='--', \n",
                "           label=f'Converged cost = {costs_conv[-1]:.6f}', alpha=0.7)\n",
                "plt.xlabel('Iteration', fontsize=12)\n",
                "plt.ylabel('Cost', fontsize=12)\n",
                "plt.title('Convergence with Stopping Criterion', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f'Total iterations: {len(costs_conv)}')\n",
                "print(f'Final θ: {theta_conv.ravel()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Analysis 3: Learning Rate Schedule\n",
                "\n",
                "Decrease learning rate over time for better convergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gd_with_schedule(X, y, lr_initial=0.5, decay=0.99, n_iter=200):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    costs = []\n",
                "    learning_rates = []\n",
                "    \n",
                "    lr = lr_initial\n",
                "    for iteration in range(n_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        theta = theta - lr * gradients\n",
                "        costs.append(compute_cost(X, y, theta))\n",
                "        learning_rates.append(lr)\n",
                "        \n",
                "        # Decay learning rate\n",
                "        lr = lr * decay\n",
                "    \n",
                "    return theta, costs, learning_rates\n",
                "\n",
                "# Compare constant vs decaying learning rate\n",
                "_, costs_const = gd_with_lr(X_b, y, lr=0.1, n_iter=200)\n",
                "_, costs_decay, lrs = gd_with_schedule(X_b, y, lr_initial=0.5, decay=0.99, n_iter=200)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Cost comparison\n",
                "axes[0].plot(costs_const, linewidth=2, label='Constant α = 0.1')\n",
                "axes[0].plot(costs_decay, linewidth=2, label='Decaying α (start=0.5)')\n",
                "axes[0].set_xlabel('Iteration', fontsize=12)\n",
                "axes[0].set_ylabel('Cost', fontsize=12)\n",
                "axes[0].set_title('Constant vs Decaying Learning Rate', fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Learning rate over time\n",
                "axes[1].plot(lrs, linewidth=2, color='orange')\n",
                "axes[1].set_xlabel('Iteration', fontsize=12)\n",
                "axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
                "axes[1].set_title('Learning Rate Decay Schedule', fontsize=13, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Benefit of Learning Rate Decay\n",
                "\n",
                "- Start with large steps (fast initial progress)\n",
                "- Gradually decrease (fine-tune near minimum)\n",
                "- Best of both worlds!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Analysis 4: Momentum\n",
                "\n",
                "Add momentum to accelerate convergence and dampen oscillations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gd_with_momentum(X, y, lr=0.01, momentum=0.9, n_iter=200):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    velocity = np.zeros_like(theta)\n",
                "    costs = []\n",
                "    \n",
                "    for _ in range(n_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        \n",
                "        # Update velocity\n",
                "        velocity = momentum * velocity + lr * gradients\n",
                "        \n",
                "        # Update theta\n",
                "        theta = theta - velocity\n",
                "        \n",
                "        costs.append(compute_cost(X, y, theta))\n",
                "    \n",
                "    return theta, costs\n",
                "\n",
                "# Compare vanilla GD vs with momentum\n",
                "_, costs_vanilla = gd_with_lr(X_b, y, lr=0.01, n_iter=200)\n",
                "_, costs_momentum = gd_with_momentum(X_b, y, lr=0.01, momentum=0.9, n_iter=200)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(costs_vanilla, linewidth=2, label='Vanilla GD', alpha=0.8)\n",
                "plt.plot(costs_momentum, linewidth=2, label='GD + Momentum', alpha=0.8)\n",
                "plt.xlabel('Iteration', fontsize=12)\n",
                "plt.ylabel('Cost', fontsize=12)\n",
                "plt.title('Gradient Descent with Momentum', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f'Vanilla GD final cost:   {costs_vanilla[-1]:.8f}')\n",
                "print(f'GD + Momentum final cost: {costs_momentum[-1]:.8f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Momentum Benefits\n",
                "\n",
                "- Faster convergence (accumulates gradients in consistent directions)\n",
                "- Dampens oscillations\n",
                "- Can escape shallow local minima\n",
                "- Standard in deep learning (β typically 0.9)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary: Convergence Best Practices\n",
                "\n",
                "### 1. Choose Learning Rate Carefully\n",
                "- Start with 0.01 or 0.1\n",
                "- Monitor cost - should decrease smoothly\n",
                "- Use grid search or adaptive methods\n",
                "\n",
                "### 2. Use Convergence Criteria\n",
                "- Don't rely only on fixed iterations\n",
                "- Check cost change: $|J^{(t)} - J^{(t-1)}| < \\epsilon$\n",
                "- Combine with max iterations as safety\n",
                "\n",
                "### 3. Consider Learning Rate Schedules\n",
                "- Step decay: reduce by factor every N epochs\n",
                "- Exponential decay: $\\alpha_t = \\alpha_0 e^{-kt}$\n",
                "- 1/t decay: $\\alpha_t = \\alpha_0 / (1 + kt)$\n",
                "\n",
                "### 4. Add Momentum\n",
                "- Especially helpful for:\n",
                "  - High-dimensional problems\n",
                "  - Ill-conditioned problems (ravines)\n",
                "  - When using SGD/mini-batch\n",
                "\n",
                "### 5. Modern Approach: Adaptive Optimizers\n",
                "- Adam (most popular)\n",
                "- RMSprop\n",
                "- AdaGrad\n",
                "- These handle learning rate automatically!\n",
                "\n",
                "---\n",
                "\n",
                "**Interview Tip**: \"For gradient descent convergence, I'd start with a learning rate of 0.01-0.1, use momentum (0.9), monitor the cost curve, and consider learning rate decay if convergence is slow. For deep learning, I'd use Adam optimizer which handles these details automatically.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}