{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Descent From Scratch\n",
                "\n",
                "Implementing all three variants:\n",
                "1. Batch Gradient Descent\n",
                "2. Stochastic Gradient Descent (SGD)\n",
                "3. Mini-Batch Gradient Descent\n",
                "\n",
                "We'll compare their behavior on the same problem.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.animation import  FuncAnimation\n",
                "from IPython.display import HTML\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup: Generate Data\n",
                "\n",
                "We'll use linear regression as our test problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data: y = 3x + 5 + noise\n",
                "m = 1000  # Large dataset to see differences\n",
                "X = 2 * np.random.rand(m, 1)\n",
                "y = 5 + 3 * X + np.random.randn(m, 1)\n",
                "\n",
                "# Add bias\n",
                "X_b = np.c_[np.ones((m, 1)), X]\n",
                "\n",
                "print(f'Dataset: {m} examples')\n",
                "print(f'True parameters: θ₀ = 5, θ₁ = 3')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Implementation 1: Batch Gradient Descent\n",
                "\n",
                "Uses ALL examples in each iteration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_cost(X, y, theta):\n",
                "    m = len(y)\n",
                "    predictions = X.dot(theta)\n",
                "    cost = (1/(2*m)) * np.sum((predictions - y)**2)\n",
                "    return cost\n",
                "\n",
                "def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    cost_history = []\n",
                "    theta_history = [theta.copy()]\n",
                "    \n",
                "    for iteration in range(n_iterations):\n",
                "        # Compute gradient using ALL examples\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        \n",
                "        # Update\n",
                "        theta = theta - learning_rate * gradients\n",
                "        \n",
                "        # Track\n",
                "        cost = compute_cost(X, y, theta)\n",
                "        cost_history.append(cost)\n",
                "        theta_history.append(theta.copy())\n",
                "    \n",
                "    return theta, cost_history, theta_history\n",
                "\n",
                "# Train\n",
                "print('Training Batch Gradient Descent...')\n",
                "theta_batch, cost_batch, theta_hist_batch = batch_gradient_descent(\n",
                "    X_b, y, learning_rate=0.1, n_iterations=500\n",
                ")\n",
                "print(f'Final θ: {theta_batch.ravel()}')\n",
                "print(f'Final cost: {cost_batch[-1]:.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Implementation 2: Stochastic Gradient Descent\n",
                "\n",
                "Uses ONE random example per iteration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=50):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    cost_history = []\n",
                "    theta_history = [theta.copy()]\n",
                "    \n",
                "    for epoch in range(n_epochs):\n",
                "        # Shuffle data\n",
                "        indices = np.random.permutation(m)\n",
                "        X_shuffled = X[indices]\n",
                "        y_shuffled = y[indices]\n",
                "        \n",
                "        for i in range(m):\n",
                "            # Use only ONE example\n",
                "            xi = X_shuffled[i:i+1]\n",
                "            yi = y_shuffled[i:i+1]\n",
                "            \n",
                "            # Compute gradient for this example\n",
                "            gradient = xi.T.dot(xi.dot(theta) - yi)\n",
                "            \n",
                "            # Update\n",
                "            theta = theta - learning_rate * gradient\n",
                "            \n",
                "            # Track (compute cost on full dataset periodically)\n",
                "            if i % 100 == 0:\n",
                "                cost = compute_cost(X, y, theta)\n",
                "                cost_history.append(cost)\n",
                "                theta_history.append(theta.copy())\n",
                "    \n",
                "    return theta, cost_history, theta_history\n",
                "\n",
                "# Train\n",
                "print('Training Stochastic Gradient Descent...')\n",
                "theta_sgd, cost_sgd, theta_hist_sgd = stochastic_gradient_descent(\n",
                "    X_b, y, learning_rate=0.01, n_epochs=10\n",
                ")\n",
                "print(f'Final θ: {theta_sgd.ravel()}')\n",
                "print(f'Final cost: {cost_sgd[-1]:.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Implementation 3: Mini-Batch Gradient Descent\n",
                "\n",
                "Uses small batches (e.g., 32 examples) per iteration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def minibatch_gradient_descent(X, y, batch_size=32, learning_rate=0.1, n_epochs=50):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    cost_history = []\n",
                "    theta_history = [theta.copy()]\n",
                "    \n",
                "    n_batches = m // batch_size\n",
                "    \n",
                "    for epoch in range(n_epochs):\n",
                "        # Shuffle data\n",
                "        indices = np.random.permutation(m)\n",
                "        X_shuffled = X[indices]\n",
                "        y_shuffled = y[indices]\n",
                "        \n",
                "        for i in range(n_batches):\n",
                "            # Get mini-batch\n",
                "            start = i * batch_size\n",
                "            end = start + batch_size\n",
                "            X_batch = X_shuffled[start:end]\n",
                "            y_batch = y_shuffled[start:end]\n",
                "            \n",
                "            # Compute gradient on batch\n",
                "            gradients = (1/batch_size) * X_batch.T.dot(X_batch.dot(theta) - y_batch)\n",
                "            \n",
                "            # Update\n",
                "            theta = theta - learning_rate * gradients\n",
                "            \n",
                "            # Track\n",
                "            if i % 5 == 0:\n",
                "                cost = compute_cost(X, y, theta)\n",
                "                cost_history.append(cost)\n",
                "                theta_history.append(theta.copy())\n",
                "    \n",
                "    return theta, cost_history, theta_history\n",
                "\n",
                "# Train\n",
                "print('Training Mini-Batch Gradient Descent...')\n",
                "theta_minibatch, cost_minibatch, theta_hist_minibatch = minibatch_gradient_descent(\n",
                "    X_b, y, batch_size=32, learning_rate=0.1, n_epochs=50\n",
                ")\n",
                "print(f'Final θ: {theta_minibatch.ravel()}')\n",
                "print(f'Final cost: {cost_minibatch[-1]:.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Comparison: Cost Over Time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Batch GD\n",
                "axes[0].plot(cost_batch, linewidth=2, color='blue')\n",
                "axes[0].set_xlabel('Iteration', fontsize=12)\n",
                "axes[0].set_ylabel('Cost', fontsize=12)\n",
                "axes[0].set_title('Batch GD: Smooth Convergence', fontsize=13, fontweight='bold')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# SGD\n",
                "axes[1].plot(cost_sgd, linewidth=2, color='red', alpha=0.7)\n",
                "axes[1].set_xlabel('Update Step', fontsize=12)\n",
                "axes[1].set_ylabel('Cost', fontsize=12)\n",
                "axes[1].set_title('SGD: Noisy, Fast', fontsize=13, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# Mini-Batch\n",
                "axes[2].plot(cost_minibatch, linewidth=2, color='green')\n",
                "axes[2].set_xlabel('Update Step', fontsize=12)\n",
                "axes[2].set_ylabel('Cost', fontsize=12)\n",
                "axes[2].set_title('Mini-Batch GD: Balanced', fontsize=13, fontweight='bold')\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'Batch GD final cost:      {cost_batch[-1]:.6f}')\n",
                "print(f'SGD final cost:           {cost_sgd[-1]:.6f}')\n",
                "print(f'Mini-Batch GD final cost: {cost_minibatch[-1]:.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Comparison: Convergence Path in Parameter Space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create cost surface\n",
                "theta0_vals = np.linspace(3, 7, 100)\n",
                "theta1_vals = np.linspace(1, 5, 100)\n",
                "theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)\n",
                "\n",
                "J_vals = np.zeros(theta0_grid.shape)\n",
                "for i in range(len(theta0_vals)):\n",
                "    for j in range(len(theta1_vals)):\n",
                "        theta_test = np.array([[theta0_grid[j, i]], [theta1_grid[j, i]]])\n",
                "        J_vals[j, i] = compute_cost(X_b, y, theta_test)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(14, 10))\n",
                "\n",
                "# Contour\n",
                "plt.contour(theta0_grid, theta1_grid, J_vals, levels=30, alpha=0.6)\n",
                "plt.colorbar(label='Cost')\n",
                "\n",
                "# Plot paths (subsample for clarity)\n",
                "batch_path = np.array(theta_hist_batch)[::10]\n",
                "sgd_path = np.array(theta_hist_sgd)[::20]\n",
                "minibatch_path = np.array(theta_hist_minibatch)[::10]\n",
                "\n",
                "plt.plot(batch_path[:, 0], batch_path[:, 1], 'b-o', \n",
                "        linewidth=2, markersize=4, label='Batch GD', alpha=0.8)\n",
                "plt.plot(sgd_path[:, 0], sgd_path[:, 1], 'r-o', \n",
                "        linewidth=2, markersize=3, label='SGD', alpha=0.6)\n",
                "plt.plot(minibatch_path[:, 0], minibatch_path[:, 1], 'g-o', \n",
                "        linewidth=2, markersize=4, label='Mini-Batch GD', alpha=0.8)\n",
                "\n",
                "# Mark optimum\n",
                "plt.scatter([5], [3], color='yellow', s=300, marker='*', \n",
                "           edgecolors='black', linewidths=2, label='True Minimum', zorder=10)\n",
                "\n",
                "plt.xlabel('θ₀ (intercept)', fontsize=13)\n",
                "plt.ylabel('θ₁ (slope)', fontsize=13)\n",
                "plt.title('Convergence Paths in Parameter Space', fontsize=15, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Observed Behavior:\n",
                "\n",
                "**Batch Gradient Descent:**\n",
                "- Smooth, direct path to minimum\n",
                "- Predictable convergence\n",
                "- Slowest (500 iterations for full dataset each time)\n",
                "\n",
                "**Stochastic Gradient Descent:**\n",
                "- Very noisy path, oscillates\n",
                "- Fast updates (see improvements quickly)\n",
                "- Never settles exactly at minimum\n",
                "\n",
                "**Mini-Batch Gradient Descent:**\n",
                "- Moderately smooth path\n",
                "- Good balance of speed and stability\n",
                "- Industry standard (PyTorch default)\n",
                "\n",
                "### Key Insights:\n",
                "\n",
                "1. **Batch GD** is great for theory and small datasets\n",
                "2. **SGD** is necessary for huge datasets and online learning\n",
                "3. **Mini-Batch** is the practical choice for almost everything\n",
                "4. All three converge to approximately the same solution\n",
                "5. Noise in SGD can help escape local minima\n",
                "\n",
                "### Interview Answer:\n",
                "\n",
                "\"For modern deep learning, I'd use mini-batch gradient descent with batch size 32-128. It's GPU-efficient, converges well, and is the industry standard. For huge datasets that don't fit in memory, I'd use SGD. Batch GD is only for educational purposes or very small datasets.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}