{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SGD vs Batch GD - Comparative Analysis\n",
                "\n",
                "Direct comparison of Stochastic vs Batch Gradient Descent:\n",
                "1. Speed comparison\n",
                "2. Convergence quality\n",
                "3. Effect of dataset size\n",
                "4. When to use each\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 5)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 1: Speed Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_cost(X, y, theta):\n",
                "    m = len(y)\n",
                "    return (1/(2*m)) * np.sum((X.dot(theta) - y)**2)\n",
                "\n",
                "def batch_gd(X, y, lr=0.1, n_iter=100):\n",
                "    m = len(y)\n",
                "    theta = np.zeros((X.shape[1], 1))\n",
                "    \n",
                "    start = time.time()\n",
                "    for _ in range(n_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        theta = theta - lr * gradients\n",
                "    elapsed = time.time() - start\n",
                "    \n",
                "    return theta, elapsed\n",
                "\n",
                "def sgd(X, y, lr=0.01, n_epochs=10):\n",
                "    m = len(y)\n",
                "    theta = np.zeros((X.shape[1], 1))\n",
                "    \n",
                "    start = time.time()\n",
                "    for epoch in range(n_epochs):\n",
                "        indices = np.random.permutation(m)\n",
                "        for i in indices:\n",
                "            xi = X[i:i+1]\n",
                "            yi = y[i:i+1]\n",
                "            gradient = xi.T.dot(xi.dot(theta) - yi)\n",
                "            theta = theta - lr * gradient\n",
                "    elapsed = time.time() - start\n",
                "    \n",
                "    return theta, elapsed\n",
                "\n",
                "# Test on different dataset sizes\n",
                "sizes = [100, 500, 1000, 5000, 10000]\n",
                "batch_times = []\n",
                "sgd_times = []\n",
                "\n",
                "for size in sizes:\n",
                "    X = 2 * np.random.rand(size, 1)\n",
                "    y = 5 + 3 * X + np.random.randn(size, 1)\n",
                "    X_b = np.c_[np.ones((size, 1)), X]\n",
                "    \n",
                "    _, t_batch = batch_gd(X_b, y, n_iter=100)\n",
                "    _, t_sgd = sgd(X_b, y, n_epochs=10)\n",
                "    \n",
                "    batch_times.append(t_batch)\n",
                "    sgd_times.append(t_sgd)\n",
                "    print(f'Size {size:5d}: Batch={t_batch:.4f}s, SGD={t_sgd:.4f}s')\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(sizes, batch_times, 'b-o', linewidth=2, markersize=8, label='Batch GD (100 iterations)')\n",
                "plt.plot(sizes, sgd_times, 'r-o', linewidth=2, markersize=8, label='SGD (10 epochs)')\n",
                "plt.xlabel('Dataset Size', fontsize=12)\n",
                "plt.ylabel('Time (seconds)', fontsize=12)\n",
                "plt.title('Speed: SGD vs Batch GD', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Observation\n",
                "\n",
                "- **Batch GD**: Time grows linearly with dataset size (processes all data each iteration)\n",
                "- **SGD**: More consistent time (processes examples independently)\n",
                "- **For large datasets**: SGD is much faster!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 2: Convergence Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fixed dataset\n",
                "m = 1000\n",
                "X = 2 * np.random.rand(m, 1)\n",
                "y = 5 + 3 * X + np.random.randn(m, 1)\n",
                "X_b = np.c_[np.ones((m, 1)), X]\n",
                "\n",
                "# Batch GD with cost tracking\n",
                "def batch_gd_track(X, y, lr=0.1, n_iter=500):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    costs = []\n",
                "    \n",
                "    for _ in range(n_iter):\n",
                "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
                "        theta = theta - lr * gradients\n",
                "        costs.append(compute_cost(X, y, theta))\n",
                "    \n",
                "    return theta, costs\n",
                "\n",
                "# SGD with cost tracking\n",
                "def sgd_track(X, y, lr=0.01, n_epochs=50):\n",
                "    m = len(y)\n",
                "    theta = np.random.randn(X.shape[1], 1)\n",
                "    costs = []\n",
                "    \n",
                "    for epoch in range(n_epochs):\n",
                "        indices = np.random.permutation(m)\n",
                "        for i in indices:\n",
                "            xi = X[i:i+1]\n",
                "            yi = y[i:i+1]\n",
                "            gradient = xi.T.dot(xi.dot(theta) - yi)\n",
                "            theta = theta - lr * gradient\n",
                "            \n",
                "            if i % 20 == 0:  # Sample cost periodically\n",
                "                costs.append(compute_cost(X, y, theta))\n",
                "    \n",
                "    return theta, costs\n",
                "\n",
                "# Train both\n",
                "theta_batch, cost_batch = batch_gd_track(X_b, y)\n",
                "theta_sgd, cost_sgd = sgd_track(X_b, y)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].plot(cost_batch, linewidth=2, color='blue')\n",
                "axes[0].set_xlabel('Iteration', fontsize=12)\n",
                "axes[0].set_ylabel('Cost', fontsize=12)\n",
                "axes[0].set_title('Batch GD: Smooth Convergence', fontsize=13, fontweight='bold')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(cost_sgd, linewidth=2, color='red', alpha=0.7)\n",
                "axes[1].set_xlabel('Update Step (sampled)', fontsize=12)\n",
                "axes[1].set_ylabel('Cost', fontsize=12)\n",
                "axes[1].set_title('SGD: Noisy but Fast', fontsize=13, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'Batch GD final: θ = {theta_batch.ravel()}, cost = {cost_batch[-1]:.6f}')\n",
                "print(f'SGD final:      θ = {theta_sgd.ravel()}, cost = {cost_sgd[-1]:.6f}')\n",
                "print(f'True parameters: θ₀ = 5, θ₁ = 3')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 3: Effect of Learning Rate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, lr in enumerate(learning_rates):\n",
                "    _, cost_batch = batch_gd_track(X_b, y, lr=lr, n_iter=200)\n",
                "    _, cost_sgd = sgd_track(X_b, y, lr=lr/10, n_epochs=20)  # SGD needs smaller LR\n",
                "    \n",
                "    axes[idx].plot(cost_batch, label='Batch GD', linewidth=2)\n",
                "    axes[idx].plot(cost_sgd, label='SGD', linewidth=2, alpha=0.7)\n",
                "    axes[idx].set_title(f'Learning Rate = {lr}', fontsize=13, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Iteration/Step', fontsize=11)\n",
                "    axes[idx].set_ylabel('Cost', fontsize=11)\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Observation\n",
                "\n",
                "- **Batch GD**: More tolerant to larger learning rates (uses exact gradient)\n",
                "- **SGD**: Needs smaller learning rates (gradient is noisy)\n",
                "- Rule of thumb: SGD learning rate ≈ 1/10 of Batch GD learning rate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Decision Guide\n",
                "\n",
                "### Use Batch Gradient Descent when:\n",
                "- Dataset is small (< 10,000 examples)\n",
                "- You need guaranteed convergence\n",
                "- You're doing educational/theoretical work\n",
                "- Memory is not a constraint\n",
                "\n",
                "### Use Stochastic Gradient Descent when:\n",
                "- Dataset is huge (millions of examples)\n",
                "- Online learning (streaming data)\n",
                "- Memory is limited\n",
                "- You need fast initial progress\n",
                "\n",
                "### Use Mini-Batch Gradient Descent when:\n",
                "- Almost any practical application!\n",
                "- Training neural networks\n",
                "- You have GPU access\n",
                "- You want balance of speed and stability\n",
                "\n",
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Speed**: SGD wins for large datasets\n",
                "2. **Stability**: Batch GD has smoother convergence\n",
                "3. **Final Quality**: Both reach similar solutions\n",
                "4. **Learning Rate**: SGD needs smaller values\n",
                "5. **Industry Standard**: Mini-batch (best of both worlds)\n",
                "\n",
                "**Interview Tip**: Always mention mini-batch as the practical choice, but explain when you'd use the extremes (batch for small data, SGD for huge/streaming data)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}