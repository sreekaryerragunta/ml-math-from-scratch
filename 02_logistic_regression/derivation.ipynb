{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression - Mathematical Derivations\n",
                "\n",
                "This notebook contains **complete mathematical derivations** for logistic regression.\n",
                "\n",
                "By the end, you'll understand:\n",
                "1. Properties of the sigmoid function\n",
                "2. Why we use log loss (cross-entropy)\n",
                "3. How to derive the gradient\n",
                "4. Why the gradient has the same form as linear regression\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Sigmoid Function Derivation\n",
                "\n",
                "### Definition:\n",
                "\n",
                "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
                "\n",
                "---\n",
                "\n",
                "### Property 1: Range is (0, 1)\n",
                "\n",
                "**When $z \\to +\\infty$**:\n",
                "$$e^{-z} \\to 0$$\n",
                "$$\\sigma(z) = \\frac{1}{1 + 0} = 1$$\n",
                "\n",
                "**When $z \\to -\\infty$**:\n",
                "$$e^{-z} \\to +\\infty$$\n",
                "$$\\sigma(z) = \\frac{1}{1 + \\infty} = 0$$\n",
                "\n",
                "**When $z = 0$**:\n",
                "$$\\sigma(0) = \\frac{1}{1 + e^0} = \\frac{1}{2}$$\n",
                "\n",
                "---\n",
                "\n",
                "### Property 2: Symmetry\n",
                "\n",
                "**Claim**: $\\sigma(-z) = 1 - \\sigma(z)$\n",
                "\n",
                "**Proof**:\n",
                "\n",
                "$$\\sigma(-z) = \\frac{1}{1 + e^{-(-z)}} = \\frac{1}{1 + e^{z}}$$\n",
                "\n",
                "Multiply numerator and denominator by $e^{-z}$:\n",
                "\n",
                "$$= \\frac{e^{-z}}{e^{-z}(1 + e^{z})} = \\frac{e^{-z}}{e^{-z} + 1}$$\n",
                "\n",
                "Now compute $1 - \\sigma(z)$:\n",
                "\n",
                "$$1 - \\sigma(z) = 1 - \\frac{1}{1 + e^{-z}} = \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = \\frac{e^{-z}}{1 + e^{-z}}$$\n",
                "\n",
                "Therefore: $\\sigma(-z) = 1 - \\sigma(z)$ ✓\n",
                "\n",
                "---\n",
                "\n",
                "### Property 3: Derivative\n",
                "\n",
                "**Claim**: $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$\n",
                "\n",
                "**Proof**:\n",
                "\n",
                "$$\\frac{d\\sigma}{dz} = \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right)$$\n",
                "\n",
                "Rewrite as: $(1 + e^{-z})^{-1}$\n",
                "\n",
                "Use chain rule:\n",
                "\n",
                "$$= -(1 + e^{-z})^{-2} \\cdot \\frac{d}{dz}(1 + e^{-z})$$\n",
                "\n",
                "$$= -(1 + e^{-z})^{-2} \\cdot (-e^{-z})$$\n",
                "\n",
                "$$= \\frac{e^{-z}}{(1 + e^{-z})^2}$$\n",
                "\n",
                "**Simplify**:\n",
                "\n",
                "$$= \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}$$\n",
                "\n",
                "$$= \\frac{1}{1 + e^{-z}} \\cdot \\left( \\frac{1 + e^{-z} - 1}{1 + e^{-z}} \\right)$$\n",
                "\n",
                "$$= \\sigma(z) \\cdot (1 - \\sigma(z))$$\n",
                "\n",
                "**Why this matters**: Makes backpropagation efficient!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Log Loss (Binary Cross-Entropy) Derivation\n",
                "\n",
                "### Why Log Loss?\n",
                "\n",
                "We want a cost function that:\n",
                "1. Penalizes wrong predictions heavily\n",
                "2. Is convex (for guaranteed convergence)\n",
                "3. Has nice derivative properties\n",
                "\n",
                "---\n",
                "\n",
                "### Maximum Likelihood Derivation\n",
                "\n",
                "Assume examples are i.i.d. (independent and identically distributed).\n",
                "\n",
                "For a single example, the probability is:\n",
                "\n",
                "$$P(y | x; \\theta) = \\begin{cases} \n",
                "h_\\theta(x) & \\text{if } y = 1 \\\\\n",
                "1 - h_\\theta(x) & \\text{if } y = 0\n",
                "\\end{cases}$$\n",
                "\n",
                "**Compact form**:\n",
                "\n",
                "$$P(y | x; \\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{1-y}$$\n",
                "\n",
                "Check: When $y=1$: $(1-h)^{1-1} = 1$, so we get $h$. When $y=0$: $h^0 = 1$, so we get $1-h$. ✓\n",
                "\n",
                "**Likelihood** (probability of all data):\n",
                "\n",
                "$$L(\\theta) = \\prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; \\theta)$$\n",
                "\n",
                "$$= \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}}$$\n",
                "\n",
                "**Log-Likelihood** (easier to work with):\n",
                "\n",
                "$$\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right]$$\n",
                "\n",
                "We want to **maximize** likelihood, which is the same as **minimizing** negative log-likelihood:\n",
                "\n",
                "$$J(\\theta) = -\\frac{1}{m} \\ell(\\theta)$$\n",
                "\n",
                "$$\\boxed{J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right]}$$\n",
                "\n",
                "This is the **log loss** (binary cross-entropy)!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Gradient Derivation\n",
                "\n",
                "### Goal: Compute $\\frac{\\partial J}{\\partial \\theta_j}$\n",
                "\n",
                "Recall:\n",
                "- $h_\\theta(x) = \\sigma(\\theta^T x)$\n",
                "- $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h^{(i)} + (1-y^{(i)}) \\log(1-h^{(i)}) \\right]$\n",
                "\n",
                "Where $h^{(i)} = h_\\theta(x^{(i)})$ for brevity.\n",
                "\n",
                "---\n",
                "\n",
                "### Step 1: Gradient of a single example\n",
                "\n",
                "$$\\frac{\\partial}{\\partial \\theta_j} \\left[ -y \\log h - (1-y) \\log(1-h) \\right]$$\n",
                "\n",
                "**Use chain rule**:\n",
                "\n",
                "$$= -y \\frac{1}{h} \\frac{\\partial h}{\\partial \\theta_j} - (1-y) \\frac{1}{1-h} \\cdot \\frac{\\partial (1-h)}{\\partial \\theta_j}$$\n",
                "\n",
                "$$= -y \\frac{1}{h} \\frac{\\partial h}{\\partial \\theta_j} - (1-y) \\frac{1}{1-h} \\cdot \\left( -\\frac{\\partial h}{\\partial \\theta_j} \\right)$$\n",
                "\n",
                "$$= -y \\frac{1}{h} \\frac{\\partial h}{\\partial \\theta_j} + (1-y) \\frac{1}{1-h} \\frac{\\partial h}{\\partial \\theta_j}$$\n",
                "\n",
                "**Factor out** $\\frac{\\partial h}{\\partial \\theta_j}$:\n",
                "\n",
                "$$= \\left( \\frac{1-y}{1-h} - \\frac{y}{h} \\right) \\frac{\\partial h}{\\partial \\theta_j}$$\n",
                "\n",
                "**Combine fractions**:\n",
                "\n",
                "$$= \\frac{h(1-y) - y(1-h)}{h(1-h)} \\frac{\\partial h}{\\partial \\theta_j}$$\n",
                "\n",
                "$$= \\frac{h - hy - y + yh}{h(1-h)} \\frac{\\partial h}{\\partial \\theta_j}$$\n",
                "\n",
                "$$= \\frac{h - y}{h(1-h)} \\frac{\\partial h}{\\partial \\theta_j}$$\n",
                "\n",
                "---\n",
                "\n",
                "### Step 2: Compute $\\frac{\\partial h}{\\partial \\theta_j}$\n",
                "\n",
                "Recall: $h = \\sigma(\\theta^T x) = \\sigma(z)$ where $z = \\theta^T x$\n",
                "\n",
                "**Use chain rule**:\n",
                "\n",
                "$$\\frac{\\partial h}{\\partial \\theta_j} = \\frac{\\partial \\sigma(z)}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j}$$\n",
                "\n",
                "We proved earlier: $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$\n",
                "\n",
                "And: $\\frac{\\partial z}{\\partial \\theta_j} = \\frac{\\partial (\\theta^T x)}{\\partial \\theta_j} = x_j$\n",
                "\n",
                "Therefore:\n",
                "\n",
                "$$\\frac{\\partial h}{\\partial \\theta_j} = \\sigma(z)(1-\\sigma(z)) \\cdot x_j = h(1-h) x_j$$\n",
                "\n",
                "---\n",
                "\n",
                "### Step 3: Combine\n",
                "\n",
                "$$\\frac{\\partial}{\\partial \\theta_j} \\left[ -y \\log h - (1-y) \\log(1-h) \\right] = \\frac{h - y}{h(1-h)} \\cdot h(1-h) x_j$$\n",
                "\n",
                "**The $h(1-h)$ terms cancel!**\n",
                "\n",
                "$$= (h - y) x_j$$\n",
                "\n",
                "---\n",
                "\n",
                "### Step 4: Full gradient\n",
                "\n",
                "$$\\boxed{\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}}$$\n",
                "\n",
                "**In vectorized form**:\n",
                "\n",
                "$$\\boxed{\\nabla J(\\theta) = \\frac{1}{m} X^T (\\sigma(X\\theta) - y)}$$\n",
                "\n",
                "---\n",
                "\n",
                "### Observation\n",
                "\n",
                "This has the **exact same form** as linear regression!\n",
                "\n",
                "**Linear Regression**: $\\nabla J = \\frac{1}{m} X^T (X\\theta - y)$\n",
                "\n",
                "**Logistic Regression**: $\\nabla J = \\frac{1}{m} X^T (\\sigma(X\\theta) - y)$\n",
                "\n",
                "The only difference: we apply sigmoid to $X\\theta$.\n",
                "\n",
                "**Why?** The sigmoid derivative's $h(1-h)$ term **cancels** with the log loss derivative, leaving the same form!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Gradient Descent Update Rule\n",
                "\n",
                "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
                "\n",
                "$$\\boxed{\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}}$$\n",
                "\n",
                "**Vectorized**:\n",
                "\n",
                "$$\\boxed{\\theta := \\theta - \\alpha \\frac{1}{m} X^T (\\sigma(X\\theta) - y)}$$\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Why Log Loss is Convex\n",
                "\n",
                "**Claim**: $J(\\theta)$ is convex when using sigmoid + log loss.\n",
                "\n",
                "**Proof sketch**:\n",
                "1. Log loss (negative log-likelihood) is convex\n",
                "2. Sigmoid is a concave function\n",
                "3. Composition of convex + concave (in the right way) yields convex\n",
                "\n",
                "**Why it matters**: Gradient descent is guaranteed to find the global minimum!\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Formulas:\n",
                "\n",
                "1. **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
                "\n",
                "2. **Hypothesis**: $h_\\theta(x) = \\sigma(\\theta^T x)$\n",
                "\n",
                "3. **Cost (Log Loss)**: $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right]$\n",
                "\n",
                "4. **Gradient**: $\\nabla J(\\theta) = \\frac{1}{m} X^T (\\sigma(X\\theta) - y)$\n",
                "\n",
                "5. **Update**: $\\theta := \\theta - \\alpha \\nabla J(\\theta)$\n",
                "\n",
                "### Beautiful Properties:\n",
                "\n",
                "- Sigmoid derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$ (elegant!)\n",
                "- Gradient has same form as linear regression (sigmoid derivative cancels!)\n",
                "- Cost function is convex (global optimum guaranteed)\n",
                "- Derived from maximum likelihood (probabilistically sound)\n",
                "\n",
                "---\n",
                "\n",
                "**Next**: Implement these equations in `logistic_regression_from_scratch.ipynb`!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}