{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression From Scratch\n",
                "\n",
                "## Objective\n",
                "\n",
                "Implement Logistic Regression using **only NumPy**. No sklearn for training!\n",
                "\n",
                "We'll build:\n",
                "1. Sigmoid function\n",
                "2. Cost function (log loss)\n",
                "3. Gradient computation\n",
                "4. Gradient Descent optimizer\n",
                "5. Prediction and evaluation\n",
                "6. Comparison with sklearn\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Generate Binary Classification Data\n",
                "\n",
                "We'll create a 2D dataset for binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate 2-class classification data\n",
                "X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
                "                          n_redundant=0, n_clusters_per_class=1,\n",
                "                          random_state=42, class_sep=1.5)\n",
                "y = y.reshape(-1, 1)  # Shape (m, 1)\n",
                "\n",
                "print(f'Dataset: {X.shape[0]} examples, {X.shape[1]} features')\n",
                "print(f'Class 0: {np.sum(y == 0)} examples')\n",
                "print(f'Class 1: {np.sum(y == 1)} examples')\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X[y.ravel() == 0, 0], X[y.ravel() == 0, 1], \n",
                "           c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
                "plt.scatter(X[y.ravel() == 1, 0], X[y.ravel() == 1, 1], \n",
                "           c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Binary Classification Dataset', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Implement Sigmoid Function\n",
                "\n",
                "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "# Visualize sigmoid\n",
                "z_range = np.linspace(-10, 10, 200)\n",
                "sig_values = sigmoid(z_range)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(z_range, sig_values, linewidth=3, color='darkblue')\n",
                "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision threshold')\n",
                "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "plt.xlabel('z', fontsize=13)\n",
                "plt.ylabel('σ(z)', fontsize=13)\n",
                "plt.title('Sigmoid (Logistic) Function', fontsize=15, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f'σ(-5) = {sigmoid(-5):.4f}')\n",
                "print(f'σ(0) = {sigmoid(0):.4f}')\n",
                "print(f'σ(5) = {sigmoid(5):.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Implement Cost Function (Log Loss)\n",
                "\n",
                "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right]$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_cost(X, y, theta):\n",
                "    m = len(y)\n",
                "    h = sigmoid(X.dot(theta))\n",
                "    \n",
                "    # Clip to avoid log(0)\n",
                "    epsilon = 1e-15\n",
                "    h = np.clip(h, epsilon, 1 - epsilon)\n",
                "    \n",
                "    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
                "    return cost"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: Implement Gradient Computation\n",
                "\n",
                "$$\\nabla J(\\theta) = \\frac{1}{m} X^T (\\sigma(X\\theta) - y)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_gradient(X, y, theta):\n",
                "    m = len(y)\n",
                "    h = sigmoid(X.dot(theta))\n",
                "    gradients = (1/m) * X.T.dot(h - y)\n",
                "    return gradients"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: Implement Gradient Descent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradient_descent(X, y, theta, learning_rate, n_iterations, verbose=True):\n",
                "    cost_history = []\n",
                "    \n",
                "    for iteration in range(n_iterations):\n",
                "        # Compute gradient\n",
                "        gradients = compute_gradient(X, y, theta)\n",
                "        \n",
                "        # Update parameters\n",
                "        theta = theta - learning_rate * gradients\n",
                "        \n",
                "        # Compute cost\n",
                "        cost = compute_cost(X, y, theta)\n",
                "        cost_history.append(cost)\n",
                "        \n",
                "        # Print progress\n",
                "        if verbose and iteration % 1000 == 0:\n",
                "            print(f'Iteration {iteration:5d} | Cost: {cost:.6f}')\n",
                "    \n",
                "    if verbose:\n",
                "        print(f'Final    | Cost: {cost:.6f}')\n",
                "    \n",
                "    return theta, cost_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 6: Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add bias term\n",
                "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
                "\n",
                "# Initialize parameters\n",
                "n_features = X_b.shape[1]\n",
                "theta_initial = np.zeros((n_features, 1))\n",
                "\n",
                "# Train\n",
                "print('='*60)\n",
                "print('GRADIENT DESCENT TRAINING')\n",
                "print('='*60)\n",
                "learning_rate = 0.1\n",
                "n_iterations = 10000\n",
                "\n",
                "theta_final, cost_history = gradient_descent(\n",
                "    X_b, y, theta_initial, learning_rate, n_iterations, verbose=True\n",
                ")\n",
                "\n",
                "print('\\nLearned parameters:'  )\n",
                "print(f'θ = {theta_final.ravel()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 7: Visualize Training Process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(cost_history, linewidth=2, color='crimson')\n",
                "plt.xlabel('Iteration', fontsize=12)\n",
                "plt.ylabel('Cost J(θ)', fontsize=12)\n",
                "plt.title('Log Loss Over Iterations', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f'Initial cost: {cost_history[0]:.6f}')\n",
                "print(f'Final cost: {cost_history[-1]:.6f}')\n",
                "print(f'Cost reduction: {((cost_history[0] - cost_history[-1]) / cost_history[0] * 100):.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 8: Make Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_proba(X, theta):\n",
                "    return sigmoid(X.dot(theta))\n",
                "\n",
                "def predict(X, theta, threshold=0.5):\n",
                "    probabilities = predict_proba(X, theta)\n",
                "    return (probabilities >= threshold).astype(int)\n",
                "\n",
                "# Get predictions\n",
                "y_pred = predict(X_b, theta_final)\n",
                "y_proba = predict_proba(X_b, theta_final)\n",
                "\n",
                "# Calculate accuracy\n",
                "accuracy = np.mean(y == y_pred)\n",
                "print(f'Training Accuracy: {accuracy * 100:.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 9: Confusion Matrix and Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute confusion matrix\n",
                "TP = np.sum((y == 1) & (y_pred == 1))\n",
                "TN = np.sum((y == 0) & (y_pred == 0))\n",
                "FP = np.sum((y == 0) & (y_pred == 1))\n",
                "FN = np.sum((y == 1) & (y_pred == 0))\n",
                "\n",
                "cm = np.array([[TN, FP], [FN, TP]])\n",
                "\n",
                "# Visualize confusion matrix\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "           xticklabels=['Pred 0', 'Pred 1'],\n",
                "           yticklabels=['True 0', 'True 1'],\n",
                "           cbar=False)\n",
                "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.show()\n",
                "\n",
                "# Calculate metrics\n",
                "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
                "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
                "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "\n",
                "print('\\nClassification Metrics:')\n",
                "print(f'Accuracy:  {accuracy:.4f}')\n",
                "print(f'Precision: {precision:.4f}')\n",
                "print(f'Recall:    {recall:.4f}')\n",
                "print(f'F1 Score:  {f1:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 10: Visualize Decision Boundary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create mesh grid\n",
                "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
                "                      np.linspace(x2_min, x2_max, 200))\n",
                "\n",
                "# Predict for each point in mesh\n",
                "mesh_points = np.c_[np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()]\n",
                "Z = predict(mesh_points, theta_final)\n",
                "Z = Z.reshape(xx1.shape)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 7))\n",
                "plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='RdYlBu', levels=[0, 0.5, 1])\n",
                "plt.scatter(X[y.ravel() == 0, 0], X[y.ravel() == 0, 1],\n",
                "           c='blue', label='Class 0', alpha=0.7, edgecolors='k', s=50)\n",
                "plt.scatter(X[y.ravel() == 1, 0], X[y.ravel() == 1, 1],\n",
                "           c='red', label='Class 1', alpha=0.7, edgecolors='k', s=50)\n",
                "\n",
                "# Plot decision boundary (where probability = 0.5)\n",
                "# θ0 + θ1*x1 + θ2*x2 = 0\n",
                "# x2 = -(θ0 + θ1*x1) / θ2\n",
                "x1_boundary = np.array([x1_min, x1_max])\n",
                "x2_boundary = -(theta_final[0] + theta_final[1] * x1_boundary) / theta_final[2]\n",
                "plt.plot(x1_boundary, x2_boundary, 'k--', linewidth=3, label='Decision Boundary')\n",
                "\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Logistic Regression Decision Boundary', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 11: Compare with Scikit-Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train sklearn model\n",
                "sklearn_model = LogisticRegression(max_iter=10000)\n",
                "sklearn_model.fit(X, y.ravel())\n",
                "\n",
                "# Get parameters\n",
                "sklearn_theta0 = sklearn_model.intercept_[0]\n",
                "sklearn_theta = np.concatenate([[sklearn_theta0], sklearn_model.coef_[0]])\n",
                "\n",
                "print('='*60)\n",
                "print('COMPARISON: OUR vs SKLEARN')\n",
                "print('='*60)\n",
                "print(f'Our θ:       {theta_final.ravel()}')\n",
                "print(f'sklearn θ:   {sklearn_theta}')\n",
                "print(f'Difference:  {np.abs(theta_final.ravel() - sklearn_theta)}')\n",
                "print('='*60)\n",
                "\n",
                "# Compare predictions\n",
                "y_pred_sklearn = sklearn_model.predict(X).reshape(-1, 1)\n",
                "accuracy_sklearn = np.mean(y == y_pred_sklearn)\n",
                "\n",
                "print(f'\\nOur Accuracy:     {accuracy:.4f}')\n",
                "print(f'sklearn Accuracy: {accuracy_sklearn:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary: What We Built\n",
                "\n",
                "### Implemented:\n",
                "1. Sigmoid function\n",
                "2. Log loss cost function\n",
                "3. Gradient computation\n",
                "4. Gradient Descent optimizer\n",
                "5. Prediction functions\n",
                "6. Evaluation metrics\n",
                "7. Decision boundary visualization\n",
                "8. Comparison with sklearn\n",
                "\n",
                "### Key Insights:\n",
                "1. **Sigmoid**: Maps any value to (0, 1) for probability interpretation\n",
                "2. **Log Loss**: Penalizes wrong predictions more heavily than MSE\n",
                "3. **Gradient**: Same form as linear regression but with sigmoid\n",
                "4. **Decision Boundary**: Where θᵀx = 0 (h(x) = 0.5)\n",
                "5. **Our implementation**: Matches sklearn!\n",
                "\n",
                "### Interview Readiness:\n",
                "You can now explain:\n",
                "- Why we use sigmoid instead of linear function\n",
                "- How to derive log loss from maximum likelihood\n",
                "- Why gradient has the same form as linear regression\n",
                "- How decision boundaries work\n",
                "- Trade-offs between different threshold values\n",
                "\n",
                "---\n",
                "\n",
                "**Next**: See `decision_boundary_visualization.ipynb` for non-linear boundaries!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}