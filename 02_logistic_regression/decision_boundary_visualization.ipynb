{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Boundary Visualization\n",
                "\n",
                "This notebook demonstrates decision boundaries for logistic regression:\n",
                "\n",
                "1. Linear decision boundaries\n",
                "2. Non-linear boundaries with polynomial features\n",
                "3. Effect of regularization on boundaries\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import make_circles, make_moons, make_classification\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "from utils import sigmoid, compute_cost, compute_gradient, gradient_descent, predict\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 5)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Scenario 1: Linearly Separable Data\n",
                "\n",
                "When data is linearly separable, a straight line can perfectly separate the classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate linearly separable data\n",
                "X_linear, y_linear = make_classification(\n",
                "    n_samples=100, n_features=2, n_informative=2,\n",
                "    n_redundant=0, n_clusters_per_class=1,\n",
                "    random_state=42, class_sep=2.0\n",
                ")\n",
                "y_linear = y_linear.reshape(-1, 1)\n",
                "\n",
                "# Add bias and train\n",
                "X_linear_b = np.c_[np.ones((X_linear.shape[0], 1)), X_linear]\n",
                "theta_linear = np.zeros((3, 1))\n",
                "theta_linear, _ = gradient_descent(X_linear_b, y_linear, theta_linear, \n",
                "                                  learning_rate=0.1, n_iterations=5000, verbose=False)\n",
                "\n",
                "# Plot\n",
                "def plot_decision_boundary(X, y, theta, title):\n",
                "    # Create mesh\n",
                "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
                "                          np.linspace(x2_min, x2_max, 200))\n",
                "    \n",
                "    # Predict for mesh\n",
                "    if theta.shape[0] == 3:  # Linear (bias + 2 features)\n",
                "        mesh_points = np.c_[np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()]\n",
                "    else:  # Polynomial features\n",
                "        mesh_points = create_poly_features(np.c_[xx1.ravel(), xx2.ravel()], degree=2)\n",
                "    \n",
                "    Z = predict(mesh_points, theta)\n",
                "    Z = Z.reshape(xx1.shape)\n",
                "    \n",
                "    # Plot\n",
                "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='RdYlBu', levels=[0, 0.5, 1])\n",
                "    plt.scatter(X[y.ravel() == 0, 0], X[y.ravel() == 0, 1],\n",
                "               c='blue', label='Class 0', alpha=0.7, edgecolors='k', s=50)\n",
                "    plt.scatter(X[y.ravel() == 1, 0], X[y.ravel() == 1, 1],\n",
                "               c='red', label='Class 1', alpha=0.7, edgecolors='k', s=50)\n",
                "    plt.xlabel('Feature 1', fontsize=11)\n",
                "    plt.ylabel('Feature 2', fontsize=11)\n",
                "    plt.title(title, fontsize=13, fontweight='bold')\n",
                "    plt.legend()\n",
                "    plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plot_decision_boundary(X_linear, y_linear, theta_linear, 'Linear Decision Boundary')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Scenario 2: Non-Linearly Separable Data (Circles)\n",
                "\n",
                "When data forms circles, a linear boundary fails. We need **polynomial features**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate circular data\n",
                "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
                "y_circles = y_circles.reshape(-1, 1)\n",
                "\n",
                "print(f'Circles dataset: {X_circles.shape}')\n",
                "print(f'Class distribution: {np.unique(y_circles, return_counts=True)}')\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_circles[y_circles.ravel() == 0, 0], X_circles[y_circles.ravel() == 0, 1],\n",
                "           c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
                "plt.scatter(X_circles[y_circles.ravel() == 1, 0], X_circles[y_circles.ravel() == 1, 1],\n",
                "           c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Circular Data (Non-Linearly Separable)', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train with Linear Features (Will Fail)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try linear logistic regression\n",
                "X_circles_b = np.c_[np.ones((X_circles.shape[0], 1)), X_circles]\n",
                "theta_circles_linear = np.zeros((3, 1))\n",
                "theta_circles_linear, _ = gradient_descent(X_circles_b, y_circles, theta_circles_linear,\n",
                "                                          learning_rate=0.1, n_iterations=5000, verbose=False)\n",
                "\n",
                "# Evaluate\n",
                "y_pred_linear = predict(X_circles_b, theta_circles_linear)\n",
                "accuracy_linear = np.mean(y_circles == y_pred_linear)\n",
                "print(f'Linear Model Accuracy: {accuracy_linear * 100:.2f}%')\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plot_decision_boundary(X_circles, y_circles, theta_circles_linear, \n",
                "                      f'Linear Boundary (Accuracy: {accuracy_linear*100:.1f}%)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Add Polynomial Features\n",
                "\n",
                "Transform: $(x_1, x_2) \\to (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$\n",
                "\n",
                "Now the decision boundary can be a **circle**!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_poly_features(X, degree=2):\n",
                "    \"\"\"\n",
                "    Create polynomial features up to specified degree.\n",
                "    Includes bias term.\n",
                "    \"\"\"\n",
                "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
                "    return poly.fit_transform(X)\n",
                "\n",
                "# Create polynomial features\n",
                "X_circles_poly = create_poly_features(X_circles, degree=2)\n",
                "print(f'Original features: {X_circles.shape}')\n",
                "print(f'With polynomial features: {X_circles_poly.shape}')\n",
                "print(f'New features: [1, x1, x2, x1^2, x1*x2, x2^2]')\n",
                "\n",
                "# Train with polynomial features\n",
                "theta_circles_poly = np.zeros((X_circles_poly.shape[1], 1))\n",
                "theta_circles_poly, cost_hist = gradient_descent(X_circles_poly, y_circles, theta_circles_poly,\n",
                "                                                learning_rate=0.1, n_iterations=10000, verbose=False)\n",
                "\n",
                "# Evaluate\n",
                "y_pred_poly = predict(X_circles_poly, theta_circles_poly)\n",
                "accuracy_poly = np.mean(y_circles == y_pred_poly)\n",
                "print(f'\\nPolynomial Model Accuracy: {accuracy_poly * 100:.2f}%')\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plot_decision_boundary(X_circles, y_circles, theta_circles_poly,\n",
                "                      f'Polynomial Boundary (Accuracy: {accuracy_poly*100:.1f}%)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Scenario 3: Moon-Shaped Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate moon data\n",
                "X_moons, y_moons = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
                "y_moons = y_moons.reshape(-1, 1)\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_moons[y_moons.ravel() == 0, 0], X_moons[y_moons.ravel() == 0, 1],\n",
                "           c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
                "plt.scatter(X_moons[y_moons.ravel() == 1, 0], X_moons[y_moons.ravel() == 1, 1],\n",
                "           c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Moon-Shaped Data', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare linear vs polynomial\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Linear\n",
                "X_moons_b = np.c_[np.ones((X_moons.shape[0], 1)), X_moons]\n",
                "theta_moons_linear = np.zeros((3, 1))\n",
                "theta_moons_linear, _ = gradient_descent(X_moons_b, y_moons, theta_moons_linear,\n",
                "                                        learning_rate=0.1, n_iterations=5000, verbose=False)\n",
                "y_pred_linear = predict(X_moons_b, theta_moons_linear)\n",
                "acc_linear = np.mean(y_moons == y_pred_linear)\n",
                "\n",
                "plt.sca(axes[0])\n",
                "plot_decision_boundary(X_moons, y_moons, theta_moons_linear,\n",
                "                      f'Linear (Acc: {acc_linear*100:.1f}%)')\n",
                "\n",
                "# Polynomial\n",
                "X_moons_poly = create_poly_features(X_moons, degree=3)\n",
                "theta_moons_poly = np.zeros((X_moons_poly.shape[1], 1))\n",
                "theta_moons_poly, _ = gradient_descent(X_moons_poly, y_moons, theta_moons_poly,\n",
                "                                      learning_rate=0.05, n_iterations=10000, verbose=False)\n",
                "y_pred_poly = predict(X_moons_poly, theta_moons_poly)\n",
                "acc_poly = np.mean(y_moons == y_pred_poly)\n",
                "\n",
                "# For polynomial boundary visualization\n",
                "x1_min, x1_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
                "x2_min, x2_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
                "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
                "                      np.linspace(x2_min, x2_max, 200))\n",
                "mesh_poly = create_poly_features(np.c_[xx1.ravel(), xx2.ravel()], degree=3)\n",
                "Z_poly = predict(mesh_poly, theta_moons_poly).reshape(xx1.shape)\n",
                "\n",
                "plt.sca(axes[1])\n",
                "plt.contourf(xx1, xx2, Z_poly, alpha=0.3, cmap='RdYlBu', levels=[0, 0.5, 1])\n",
                "plt.scatter(X_moons[y_moons.ravel() == 0, 0], X_moons[y_moons.ravel() == 0, 1],\n",
                "           c='blue', label='Class 0', alpha=0.7, edgecolors='k', s=50)\n",
                "plt.scatter(X_moons[y_moons.ravel() == 1, 0], X_moons[y_moons.ravel() == 1, 1],\n",
                "           c='red', label='Class 1', alpha=0.7, edgecolors='k', s=50)\n",
                "plt.xlabel('Feature 1', fontsize=11)\n",
                "plt.ylabel('Feature 2', fontsize=11)\n",
                "plt.title(f'Polynomial Degree 3 (Acc: {acc_poly*100:.1f}%)', fontsize=13, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'Linear Accuracy: {acc_linear*100:.2f}%')\n",
                "print(f'Polynomial Accuracy: {acc_poly*100:.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Key Insights\n",
                "\n",
                "### Linear Decision Boundaries\n",
                "- Work well for **linearly separable** data\n",
                "- Equation: $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$\n",
                "- Fast to train, interpretable\n",
                "\n",
                "### Non-Linear Decision Boundaries\n",
                "- Required for complex patterns (circles, moons, etc.)\n",
                "- Achieved by adding **polynomial features**\n",
                "- Higher degree → more complex boundaries\n",
                "- **Caution**: Too high degree → overfitting!\n",
                "\n",
                "### Trade-offs\n",
                "- **Linear**: Simple, fast, interpretable, but limited expressiveness\n",
                "- **Polynomial**: Flexible, powerful, but risk of overfitting\n",
                "- **Solution**: Use regularization (Ridge/Lasso) to control complexity\n",
                "\n",
                "### When to Use What?\n",
                "- **Linear**: When data is roughly linearly separable\n",
                "- **Polynomial (degree 2-3)**: When you see curved patterns\n",
                "- **Higher degrees or Neural Networks**: Very complex non-linear patterns\n",
                "\n",
                "---\n",
                "\n",
                "**Interview Question**: \"How would you handle non-linearly separable data in logistic regression?\"\n",
                "\n",
                "**Answer**: \"Add polynomial or interaction features to transform the feature space, making it linearly separable in the higher-dimensional space. However, I'd be careful about overfitting and would use regularization or cross-validation to choose the right polynomial degree.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}