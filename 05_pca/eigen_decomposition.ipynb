{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Eigen Decomposition\n",
                "\n",
                "Understanding eigenvalues and eigenvectors - the foundation of PCA:\n",
                "1. What are eigenvalues and eigenvectors?\n",
                "2. Geometric interpretation\n",
                "3. Computing them with NumPy\n",
                "4. Connection to variance\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Definition\n",
                "\n",
                "For a square matrix $A$, an eigenvector $v$ and eigenvalue $\\lambda$ satisfy:\n",
                "\n",
                "$$Av = \\lambda v$$\n",
                "\n",
                "**Meaning**: $v$ is a direction that $A$ only **scales** (doesn't rotate)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Example 1: Simple 2x2 Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a 2x2 matrix\n",
                "A = np.array([[3, 1],\n",
                "              [1, 3]])\n",
                "\n",
                "print('Matrix A:')\n",
                "print(A)\n",
                "\n",
                "# Compute eigenvalues and eigenvectors\n",
                "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
                "\n",
                "print(f'\\nEigenvalues: {eigenvalues}')\n",
                "print(f'\\nEigenvectors:\\n{eigenvectors}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Verify: Av = λv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First eigenvector\n",
                "v1 = eigenvectors[:, 0]\n",
                "lambda1 = eigenvalues[0]\n",
                "\n",
                "# Compute Av\n",
                "Av = A.dot(v1)\n",
                "\n",
                "# Compute λv  \n",
                "lambda_v = lambda1 * v1\n",
                "\n",
                "print(f'v1 = {v1}')\n",
                "print(f'λ1 = {lambda1:.4f}')\n",
                "print(f'\\nAv1 = {Av}')\n",
                "print(f'λ1 * v1 = {lambda_v}')\n",
                "print(f'\\nEqual? {np.allclose(Av, lambda_v)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Visualization: What Eigenvectors Mean"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Left: Before transformation\n",
                "axes[0].quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', width=0.01, label=f'v1 (λ={lambda1:.1f})')\n",
                "v2 = eigenvectors[:, 1]\n",
                "lambda2 = eigenvalues[1]\n",
                "axes[0].quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', width=0.01, label=f'v2 (λ={lambda2:.1f})')\n",
                "axes[0].set_xlim(-1, 4)\n",
                "axes[0].set_ylim(-1, 4)\n",
                "axes[0].set_aspect('equal')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "axes[0].set_title('Eigenvectors (Before)', fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
                "\n",
                "# Right: After transformation\n",
                "Av1 = A.dot(v1)\n",
                "Av2 = A.dot(v2)\n",
                "axes[1].quiver(0, 0, Av1[0], Av1[1], angles='xy', scale_units='xy', scale=1, color='r', width=0.01, label=f'Av1 = {lambda1:.1f}*v1')\n",
                "axes[1].quiver(0, 0, Av2[0], Av2[1], angles='xy', scale_units='xy', scale=1, color='b', width=0.01, label=f'Av2 = {lambda2:.1f}*v2')\n",
                "axes[1].set_xlim(-1, 4)\n",
                "axes[1].set_ylim(-1, 4)\n",
                "axes[1].set_aspect('equal')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "axes[1].set_title('After A Transformation', fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation**: Eigenvectors stay in same direction, only get scaled!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Covariance Matrix and Variance\n",
                "\n",
                "For PCA, we use eigendecomposition on the **covariance matrix**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate correlated 2D data\n",
                "mean = [0, 0]\n",
                "cov = [[3, 1], \n",
                "       [1, 3]]  # This is our matrix A from earlier!\n",
                "\n",
                "data = np.random.multivariate_normal(mean, cov, 500)\n",
                "\n",
                "# Compute covariance matrix\n",
                "cov_matrix = np.cov(data.T)\n",
                "\n",
                "print('True covariance:')\n",
                "print(np.array(cov))\n",
                "print('\\nEstimated covariance:')\n",
                "print(cov_matrix)\n",
                "\n",
                "# Eigendecomposition\n",
                "eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n",
                "print(f'\\nEigenvalues (variances): {eigenvals}')\n",
                "print(f'\\nEigenvectors (PC directions):\\n{eigenvecs}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Data with Principal Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, label='Data')\n",
                "\n",
                "# Plot eigenvectors scaled by eigenvalues (show variance)\n",
                "origin = np.mean(data, axis=0)\n",
                "scale = 2  # For visibility\n",
                "\n",
                "for i in range(2):\n",
                "    direction = eigenvecs[:, i] * np.sqrt(eigenvals[i]) * scale\n",
                "    plt.arrow(origin[0], origin[1], direction[0], direction[1],\n",
                "             head_width=0.3, head_length=0.4, fc=f'C{i}', ec=f'C{i}',\n",
                "             linewidth=3, label=f'PC{i+1} (λ={eigenvals[i]:.2f})')\n",
                "\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Data with Principal Components', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpretation:\n",
                "- **PC1** (larger eigenvalue): Direction of maximum variance\n",
                "- **PC2** (smaller eigenvalue): Direction of second-most variance\n",
                "- Eigenvectors are **orthogonal** (perpendicular)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Key Properties\n",
                "\n",
                "### 1. Eigenvectors are Orthogonal (for symmetric matrices)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dot product of eigenvectors\n",
                "dot_product = eigenvecs[:, 0].dot(eigenvecs[:, 1])\n",
                "print(f'v1 · v2 = {dot_product:.10f}  (should be ~0)')\n",
                "print(f'Orthogonal? {np.abs(dot_product) < 1e-10}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Eigenvalues Represent Variance in Each Direction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Project data onto principal components\n",
                "data_centered = data - np.mean(data, axis=0)\n",
                "projected = data_centered.dot(eigenvecs)\n",
                "\n",
                "# Compute variance in each PC direction\n",
                "var_pc1 = np.var(projected[:, 0])\n",
                "var_pc2 = np.var(projected[:, 1])\n",
                "\n",
                "print(f'Eigenvalue 1: {eigenvals[0]:.4f}')\n",
                "print(f'Variance along PC1: {var_pc1:.4f}')\n",
                "print(f'\\nEigenvalue 2: {eigenvals[1]:.4f}')\n",
                "print(f'Variance along PC2: {var_pc2:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Eigendecomposition:\n",
                "$$A v = \\lambda v$$\n",
                "\n",
                "- **Eigenvector** ($v$): Direction unchanged by transformation\n",
                "- **Eigenvalue** ($\\lambda$): Scaling factor\n",
                "\n",
                "### For PCA:\n",
                "1. Compute covariance matrix of data\n",
                "2. Find eigenvalues and eigenvectors\n",
                "3. **Eigenvectors** = Principal components (new axes)\n",
                "4. **Eigenvalues** = Variance along each PC\n",
                "5. Sort by eigenvalue (descending)\n",
                "6. Keep top k eigenvectors for dimensionality reduction\n",
                "\n",
                "### Why It Works:\n",
                "- Eigenvectors of covariance matrix point in directions of maximum variance\n",
                "- Data projected onto these directions preserves most information\n",
                "- Can discard components with small eigenvalues (low variance)\n",
                "\n",
                "**Interview Tip**: \"Eigenvectors of the covariance matrix are the principal components - they're the directions where data varies most. Eigenvalues tell us how much variance each component captures. PCA keeps the top eigenvectors to reduce dimensions while preserving information.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}