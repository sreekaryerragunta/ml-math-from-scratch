{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reconstruction Error Analysis\n",
                "\n",
                "Analyzing information loss in PCA:\n",
                "1. Reconstruction error vs number of components\n",
                "2. Visualizing the tradeoff\n",
                "3. Choosing optimal k\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.datasets import fetch_olivetti_faces, load_digits\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Understanding Reconstruction Error\n",
                "\n",
                "When we reduce dimensions, we lose information. How much?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def reconstruction_error(X_original, X_reconstructed):\n",
                "    \"\"\"Mean squared error between original and reconstructed data.\"\"\"\n",
                "    return np.mean((X_original - X_reconstructed)**2)\n",
                "\n",
                "def compute_errors_for_components(X, max_components):\n",
                "    \"\"\"Compute reconstruction error for different numbers of components.\"\"\"\n",
                "    errors = []\n",
                "    var_explained = []\n",
                "    \n",
                "    for n_comp in range(1, max_components + 1):\n",
                "        pca = PCA(n_components=n_comp)\n",
                "        X_transformed = pca.fit_transform(X)\n",
                "        X_reconstructed = pca.inverse_transform(X_transformed)\n",
                "        \n",
                "        error = reconstruction_error(X, X_reconstructed)\n",
                "        var_exp = np.sum(pca.explained_variance_ratio_)\n",
                "        \n",
                "        errors.append(error)\n",
                "        var_explained.append(var_exp)\n",
                "    \n",
                "    return errors, var_explained"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 1: Digits Dataset (8x8 images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load digits\n",
                "digits = load_digits()\n",
                "X_digits = digits.data\n",
                "\n",
                "print(f'Digits dataset: {X_digits.shape}')\n",
                "print(f'Each digit: 8x8 = 64 pixels')\n",
                "\n",
                "# Compute errors\n",
                "max_comp = min(64, X_digits.shape[0])  # Can't exceed min dimension\n",
                "errors_digits, var_exp_digits = compute_errors_for_components(X_digits, max_comp)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].plot(range(1, max_comp + 1), errors_digits, linewidth=2)\n",
                "axes[0].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[0].set_ylabel('Reconstruction Error (MSE)', fontsize=12)\n",
                "axes[0].set_title('Reconstruction Error vs Components (Digits)', fontsize=13, fontweight='bold')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(range(1, max_comp + 1), var_exp_digits, linewidth=2, color='orange')\n",
                "axes[1].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% variance')\n",
                "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[1].set_ylabel('Variance Explained', fontsize=12)\n",
                "axes[1].set_title('Variance Explained vs Components', fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Find optimal\n",
                "n_95 = np.argmax(np.array(var_exp_digits) >= 0.95) + 1\n",
                "print(f'\\nFor 95% variance: {n_95} components needed')\n",
                "print(f'Reconstruction error at 95%: {errors_digits[n_95-1]:.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 2: Visual Reconstruction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pick one digit\n",
                "digit_idx = 10\n",
                "original_digit = X_digits[digit_idx].reshape(8, 8)\n",
                "\n",
                "# Reconstruct with different components\n",
                "components_to_test = [2, 5, 10, 20, 30, 64]\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for i, n_comp in enumerate(components_to_test):\n",
                "    if n_comp > max_comp:\n",
                "        n_comp = max_comp\n",
                "    \n",
                "    pca = PCA(n_components=n_comp)\n",
                "    digit_encoded = pca.fit_transform(X_digits[digit_idx:digit_idx+1])\n",
                "    digit_reconstructed = pca.inverse_transform(digit_encoded).reshape(8, 8)\n",
                "    \n",
                "    error = reconstruction_error( \n",
                "        original_digit.flatten().reshape(1, -1),\n",
                "        digit_reconstructed.flatten().reshape(1, -1)\n",
                "    )\n",
                "    var_exp = np.sum(pca.explained_variance_ratio_) * 100\n",
                "    \n",
                "    axes[i].imshow(digit_reconstructed, cmap='gray')\n",
                "    axes[i].set_title(f'{n_comp} comp | Error: {error:.4f}\\n{var_exp:.1f}% variance', fontsize=10)\n",
                "    axes[i].axis('off')\n",
                "\n",
                "plt.suptitle(f'Digit Reconstruction (Original label: {digits.target[digit_idx]})', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 3: Faces (Higher Dimensional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load faces\n",
                "faces_data = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
                "X_faces = faces_data.data[:100]  # Use subset for speed\n",
                "\n",
                "print(f'Faces dataset: {X_faces.shape}')\n",
                "print(f'Each face: 64x64 = 4096 pixels')\n",
                "\n",
                "# Test different numbers of components\n",
                "components_range = list(range(5, 101, 5))\n",
                "errors_faces = []\n",
                "var_exp_faces = []\n",
                "\n",
                "for n_comp in components_range:\n",
                "    pca = PCA(n_components=n_comp)\n",
                "    X_transformed = pca.fit_transform(X_faces)\n",
                "    X_reconstructed = pca.inverse_transform(X_transformed)\n",
                "    \n",
                "    error = reconstruction_error(X_faces, X_reconstructed)\n",
                "    var_exp = np.sum(pca.explained_variance_ratio_)\n",
                "    \n",
                "    errors_faces.append(error)\n",
                "    var_exp_faces.append(var_exp)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].plot(components_range, errors_faces, 'o-', linewidth=2, markersize=5)\n",
                "axes[0].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[0].set_ylabel('Reconstruction Error', fontsize=12)\n",
                "axes[0].set_title('Reconstruction Error vs Components (Faces)', fontsize=13, fontweight='bold')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(components_range, var_exp_faces, 'o-', linewidth=2, markersize=5, color='green')\n",
                "axes[1].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% variance')\n",
                "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[1].set_ylabel('Variance Explained', fontsize=12)\n",
                "axes[1].set_title('Variance Explained vs Components', fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Choosing Optimal k\n",
                "\n",
                "### Method 1: Variance Threshold\n",
                "Keep components until reaching desired variance (e.g., 95%).\n",
                "\n",
                "### Method 2: Elbow Method  \n",
                "Look for \"elbow\" in reconstruction error curve.\n",
                "\n",
                "### Method 3: Application-Specific\n",
                "Based on downstream task performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find different thresholds\n",
                "thresholds = [0.90, 0.95, 0.99]\n",
                "\n",
                "for threshold in thresholds:\n",
                "    n_comp = np.argmax(np.array(var_exp_faces) >= threshold)\n",
                "    if n_comp == 0 and var_exp_faces[0] < threshold:\n",
                "        print(f'{threshold*100:.0f}% variance: Need more than {max(components_range)} components')\n",
                "    else:\n",
                "        actual_comp = components_range[n_comp]\n",
                "        error =errors_faces[n_comp]\n",
                "        compression = (1 - actual_comp / 4096) * 100\n",
                "        print(f'{threshold*100:.0f}% variance: {actual_comp} components | Error: {error:.6f} | Compression: {compression:.1f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Reconstruction Error:\n",
                "- Decreases as we add more components\n",
                "- Zero error only with k = original dimensions\n",
                "- Tradeoff between compression and accuracy\n",
                "\n",
                "### Key Insights:\n",
                "1. **First few PCs capture most information** (80-90% with <20% of dims)\n",
                "2. **Diminishing returns** - later PCs add little value\n",
                "3. **Elbow in error curve** - good indicator of optimal k\n",
                "4. **95% variance** is common practical threshold\n",
                "\n",
                "### Choosing k:\n",
                "- **Variance threshold**: e.g., keep 95% of variance\n",
                "- **Elbow method**: Look for bend in error curve\n",
                "- **Cross-validation**: Test downstream task performance\n",
                "- **Computational budget**: Balance accuracy vs speed\n",
                "\n",
                "### Error Formula:\n",
                "$$\\text{Reconstruction Error} = ||X - \\hat{X}||^2$$\n",
                "\n",
                "Where $\\hat{X} = X_{PCA} W^T$ (reconstructed data)\n",
                "\n",
                "### In Practice:\n",
                "- Start with high k (e.g., 95% variance)\n",
                "- Evaluate on your task\n",
                "- Reduce k if performance doesn't suffer\n",
                "- Monitor train vs test performance\n",
                "\n",
                "**Interview Tip**: \"Reconstruction error measures information loss from dimensionality reduction. It decreases with more components, but we see diminishing returns. I'd use the 95% variance threshold as a starting point, then validate with cross-validation on the actual task. The elbow in the error curve also helps identify a good k.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}