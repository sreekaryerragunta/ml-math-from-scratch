{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# L1 and L2 Regularization From Scratch\n",
                "\n",
                "Implementing Ridge (L2) and Lasso (L1) regression from scratch:\n",
                "1. Ridge Regression (L2 penalty)\n",
                "2. Lasso Regression (L1 penalty)\n",
                "3. Comparison with standard linear regression\n",
                "4. Effect on coefficients\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Generate Data with Noise\n",
                "\n",
                "Create dataset with many features, some irrelevant."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "n_samples = 100\n",
                "n_features = 20\n",
                "\n",
                "# True coefficients (sparse - many zeros)\n",
                "true_coef = np.zeros(n_features)\n",
                "true_coef[0] = 5.0\n",
                "true_coef[2] = 3.0\n",
                "true_coef[5] = -2.0\n",
                "true_coef[10] = 1.5\n",
                "\n",
                "X = np.random.randn(n_samples, n_features)\n",
                "y = X.dot(true_coef) + np.random.randn(n_samples) * 0.5\n",
                "\n",
                "# Standardize features\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "print(f'Dataset: {n_samples} samples, {n_features} features')\n",
                "print(f'True non-zero coefficients: {np.sum(true_coef != 0)}')\n",
                "print(f'True coefficients:\\n{true_coef}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Implementation 1: Ridge Regression (L2)\n",
                "\n",
                "Cost: $J(\\theta) = \\frac{1}{2m}||X\\theta - y||^2 + \\frac{\\lambda}{2m}||\\theta||^2$\n",
                "\n",
                "Closed form solution: $\\theta = (X^TX + \\lambda I)^{-1}X^Ty$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ridge_regression(X, y, lambda_reg):\n",
                "    \"\"\"\n",
                "    Ridge regression using normal equation.\n",
                "    \n",
                "    Cost: J = (1/2m)||Xθ - y||^2 + (λ/2m)||θ||^2\n",
                "    Solution: θ = (X^T X + λI)^(-1) X^T y\n",
                "    \"\"\"\n",
                "    m, n = X.shape\n",
                "    identity = np.eye(n)\n",
                "    \n",
                "    # Normal equation with L2 penalty\n",
                "    theta = np.linalg.inv(X.T.dot(X) + lambda_reg * identity).dot(X.T).dot(y)\n",
                "    return theta\n",
                "\n",
                "# Test different lambda values\n",
                "lambdas = [0, 0.1, 1.0, 10.0, 100.0]\n",
                "ridge_coefs = []\n",
                "\n",
                "for lam in lambdas:\n",
                "    coef = ridge_regression(X_scaled, y, lam)\n",
                "    ridge_coefs.append(coef)\n",
                "    print(f'λ = {lam:6.1f}: ||θ||^2 = {np.sum(coef**2):.4f}, non-zeros = {np.sum(np.abs(coef) > 0.01)}')\n",
                "\n",
                "ridge_coefs = np.array(ridge_coefs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Ridge Coefficients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "for i, lam in enumerate(lambdas):\n",
                "    plt.plot(ridge_coefs[i], marker='o', label=f'λ = {lam}', alpha=0.7)\n",
                "\n",
                "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
                "plt.xlabel('Feature Index', fontsize=12)\n",
                "plt.ylabel('Coefficient Value', fontsize=12)\n",
                "plt.title('Ridge Regression: Effect of λ on Coefficients', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation**: As λ increases, all coefficients shrink toward zero, but none become exactly zero."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Implementation 2: Lasso Regression (L1)\n",
                "\n",
                "Cost: $J(\\theta) = \\frac{1}{2m}||X\\theta - y||^2 + \\frac{\\lambda}{m}||\\theta||_1$\n",
                "\n",
                "No closed form - use coordinate descent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def soft_threshold(x, lambda_val):\n",
                "    \"\"\"Soft thresholding operator for Lasso.\"\"\"\n",
                "    if x > lambda_val:\n",
                "        return x - lambda_val\n",
                "    elif x < -lambda_val:\n",
                "        return x + lambda_val\n",
                "    else:\n",
                "        return 0.0\n",
                "\n",
                "def lasso_regression(X, y, lambda_reg, max_iter=1000, tol=1e-4):\n",
                "    \"\"\"\n",
                "    Lasso regression using coordinate descent.\n",
                "    \"\"\"\n",
                "    m, n = X.shape\n",
                "    theta = np.zeros(n)\n",
                "    \n",
                "    for iteration in range(max_iter):\n",
                "        theta_old = theta.copy()\n",
                "        \n",
                "        for j in range(n):\n",
                "            # Compute residual without feature j\n",
                "            residual = y - X.dot(theta) + X[:, j] * theta[j]\n",
                "            \n",
                "            # Compute correlation\n",
                "            rho = X[:, j].dot(residual)\n",
                "            \n",
                "            # Update with soft thresholding\n",
                "            theta[j] = soft_threshold(rho, lambda_reg * m) / (X[:, j].dot(X[:, j]))\n",
                "        \n",
                "        # Check convergence\n",
                "        if np.sum(np.abs(theta - theta_old)) < tol:\n",
                "            break\n",
                "    \n",
                "    return theta\n",
                "\n",
                "# Test different lambda values\n",
                "lasso_coefs = []\n",
                "\n",
                "for lam in lambdas:\n",
                "    coef = lasso_regression(X_scaled, y, lam)\n",
                "    lasso_coefs.append(coef)\n",
                "    print(f'λ = {lam:6.1f}: ||θ||_1 = {np.sum(np.abs(coef)):.4f}, non-zeros = {np.sum(np.abs(coef) > 0.01)}')\n",
                "\n",
                "lasso_coefs = np.array(lasso_coefs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Lasso Coefficients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "for i, lam in enumerate(lambdas):\n",
                "    plt.plot(lasso_coefs[i], marker='o', label=f'λ = {lam}', alpha=0.7)\n",
                "\n",
                "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
                "plt.xlabel('Feature Index', fontsize=12)\n",
                "plt.ylabel('Coefficient Value', fontsize=12)\n",
                "plt.title('Lasso Regression: Effect of λ on Coefficients (Sparsity!)', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation**: As λ increases, many coefficients become exactly zero (sparse solution)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Comparison: Ridge vs Lasso"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Ridge path\n",
                "axes[0].plot(true_coef, 'ko', markersize=10, label='True', zorder=10)\n",
                "for i, lam in enumerate(lambdas[1:]):\n",
                "    axes[0].plot(ridge_coefs[i+1], marker='o', label=f'λ={lam}', alpha=0.7)\n",
                "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
                "axes[0].set_xlabel('Feature Index', fontsize=12)\n",
                "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
                "axes[0].set_title('Ridge (L2): Shrinkage', fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Lasso path\n",
                "axes[1].plot(true_coef, 'ko', markersize=10, label='True', zorder=10)\n",
                "for i, lam in enumerate(lambdas[1:]):\n",
                "    axes[1].plot(lasso_coefs[i+1], marker='o', label=f'λ={lam}', alpha=0.7)\n",
                "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
                "axes[1].set_xlabel('Feature Index', fontsize=12)\n",
                "axes[1].set_ylabel('Coefficient Value', fontsize=12)\n",
                "axes[1].set_title('Lasso (L1): Sparsity', fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Compare with Scikit-Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Our Ridge\n",
                "our_ridge = ridge_regression(X_scaled, y, lambda_reg=1.0)\n",
                "\n",
                "# sklearn Ridge\n",
                "sklearn_ridge = Ridge(alpha=1.0, fit_intercept=False)\n",
                "sklearn_ridge.fit(X_scaled, y)\n",
                "\n",
                "print('Ridge Comparison (λ=1.0):')\n",
                "print(f'Our implementation:   {our_ridge[:5]} ...')\n",
                "print(f'sklearn:              {sklearn_ridge.coef_[:5]} ...')\n",
                "print(f'Max difference:       {np.max(np.abs(our_ridge - sklearn_ridge.coef_)):.6f}')\n",
                "\n",
                "print('\\nLasso Comparison (λ=1.0):')\n",
                "our_lasso = lasso_regression(X_scaled, y, lambda_reg=1.0)\n",
                "sklearn_lasso = Lasso(alpha=1.0, fit_intercept=False)\n",
                "sklearn_lasso.fit(X_scaled, y)\n",
                "\n",
                "print(f'Our implementation:   {our_lasso[:5]} ...')\n",
                "print(f'sklearn:              {sklearn_lasso.coef_[:5]} ...')\n",
                "print(f'Max difference:       {np.max(np.abs(our_lasso - sklearn_lasso.coef_)):.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Ridge Regression (L2):\n",
                "- Adds $\\frac{\\lambda}{2m}\\sum\\theta_j^2$ to cost\n",
                "- Shrinks all coefficients toward zero\n",
                "- Never produces exact zeros\n",
                "- Has closed-form solution\n",
                "- Handles multicollinearity well\n",
                "\n",
                "### Lasso Regression (L1):\n",
                "- Adds $\\frac{\\lambda}{m}\\sum|\\theta_j|$ to cost\n",
                "- Forces some coefficients to exactly zero\n",
                "- Automatic feature selection\n",
                "- Requires iterative solution (coordinate descent)\n",
                "- Creates sparse models\n",
                "\n",
                "### When to Use:\n",
                "- **Ridge**: All features are potentially useful, want to reduce overfitting\n",
                "- **Lasso**: Suspect many features are irrelevant, want automatic selection\n",
                "- **Elastic Net**: Want benefits of both (compromise)\n",
                "\n",
                "**Interview Tip**: \"Ridge shrinks coefficients smoothly, Lasso forces some to zero for feature selection. I'd use cross-validation to choose between them based on which gives better validation performance.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}