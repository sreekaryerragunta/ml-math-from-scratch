{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Bias-Variance Tradeoff\n",
                "\n",
                "Demonstrating the fundamental tradeoff in machine learning:\n",
                "1. Understanding bias and variance\n",
                "2. Visualizing the tradeoff\n",
                "3. Effect of model complexity\n",
                "4. Effect of regularization\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Generate True Function\n",
                "\n",
                "We'll use a simple non-linear function and add noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# True function: quadratic\n",
                "def true_function(x):\n",
                "    return 0.5 * x**2 + x + 2\n",
                "\n",
                "# Generate data\n",
                "n_samples = 50\n",
                "X = np.linspace(-3, 3, n_samples)\n",
                "y_true = true_function(X)\n",
                "y_noisy = y_true + np.random.randn(n_samples) * 0.5\n",
                "\n",
                "# Test points for visualization\n",
                "X_test = np.linspace(-3, 3, 200)\n",
                "y_test_true = true_function(X_test)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(X, y_noisy, alpha=0.6, label='Training Data')\n",
                "plt.plot(X_test, y_test_true, 'r-', linewidth=2, label='True Function')\n",
                "plt.xlabel('x', fontsize=12)\n",
                "plt.ylabel('y', fontsize=12)\n",
                "plt.title('Data Generation', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 1: Model Complexity\n",
                "\n",
                "Compare polynomial models of different degrees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "degrees = [1, 2, 5, 15]\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, degree in enumerate(degrees):\n",
                "    # Fit polynomial\n",
                "    poly = PolynomialFeatures(degree=degree)\n",
                "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
                "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
                "    \n",
                "    # Train\n",
                "    theta = np.linalg.lstsq(X_poly, y_noisy, rcond=None)[0]\n",
                "    y_pred = X_test_poly.dot(theta)\n",
                "    \n",
                "    # Calculate train MSE\n",
                "    train_pred = X_poly.dot(theta)\n",
                "    train_mse = np.mean((train_pred - y_noisy)**2)\n",
                "    \n",
                "    # Plot\n",
                "    axes[idx].scatter(X, y_noisy, alpha=0.6, label='Data')\n",
                "    axes[idx].plot(X_test, y_test_true, 'r--', linewidth=2, label='True', alpha=0.7)\n",
                "    axes[idx].plot(X_test, y_pred, 'b-', linewidth=2, label=f'Degree {degree}')\n",
                "    axes[idx].set_xlabel('x', fontsize=11)\n",
                "    axes[idx].set_ylabel('y', fontsize=11)\n",
                "    axes[idx].set_title(f'Degree {degree} (Train MSE: {train_mse:.4f})', fontsize=12, fontweight='bold')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "    axes[idx].set_ylim(-2, 10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Observations:\n",
                "- **Degree 1**: High bias - underfits (too simple)\n",
                "- **Degree 2**: Just right - captures true pattern\n",
                "- **Degree 5**: Starting to overfit\n",
                "- **Degree 15**: High variance - overfits (wiggly, follows noise)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 2: Bias-Variance with Multiple Datasets\n",
                "\n",
                "Generate multiple datasets to see variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_datasets = 50\n",
                "degree_to_test = 15\n",
                "\n",
                "# Store predictions for each dataset\n",
                "all_predictions = []\n",
                "\n",
                "for i in range(n_datasets):\n",
                "    # Generate new noisy data\n",
                "    y_i = y_true + np.random.randn(n_samples) *0.5\n",
                "    \n",
                "    # Fit model\n",
                "    poly = PolynomialFeatures(degree=degree_to_test)\n",
                "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
                "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
                "    \n",
                "    theta = np.linalg.lstsq(X_poly, y_i, rcond=None)[0]\n",
                "    y_pred = X_test_poly.dot(theta)\n",
                "    all_predictions.append(y_pred)\n",
                "\n",
                "all_predictions = np.array(all_predictions)\n",
                "\n",
                "# Compute bias and variance\n",
                "mean_prediction = np.mean(all_predictions, axis=0)\n",
                "bias_squared = (mean_prediction - y_test_true)**2\n",
                "variance = np.var(all_predictions, axis=0)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Left: All predictions\n",
                "for pred in all_predictions[:20]:  # Show first 20\n",
                "    axes[0].plot(X_test, pred, 'b-', alpha=0.1)\n",
                "axes[0].plot(X_test, y_test_true, 'r-', linewidth=3, label='True Function')\n",
                "axes[0].plot(X_test, mean_prediction, 'g-', linewidth=3, label='Mean Prediction')\n",
                "axes[0].set_xlabel('x', fontsize=12)\n",
                "axes[0].set_ylabel('y', fontsize=12)\n",
                "axes[0].set_title(f'Variance in Predictions (Degree {degree_to_test})', fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "axes[0].set_ylim(-2, 10)\n",
                "\n",
                "# Right: Bias and Variance\n",
                "axes[1].plot(X_test, bias_squared, linewidth=2, label='Bias²')\n",
                "axes[1].plot(X_test, variance, linewidth=2, label='Variance')\n",
                "axes[1].plot(X_test, bias_squared + variance, linewidth=2, label='Total Error', linestyle='--')\n",
                "axes[1].set_xlabel('x', fontsize=12)\n",
                "axes[1].set_ylabel('Error', fontsize=12)\n",
                "axes[1].set_title('Bias² and Variance Decomposition', fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'Mean Bias²: {np.mean(bias_squared):.4f}')\n",
                "print(f'Mean Variance: {np.mean(variance):.4f}')\n",
                "print(f'Total Error: {np.mean(bias_squared + variance):.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 3: Bias-Variance vs Model Complexity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "degrees_range = range(1, 16)\n",
                "bias_list = []\n",
                "variance_list = []\n",
                "total_error_list = []\n",
                "\n",
                "for degree in degrees_range:\n",
                "    predictions = []\n",
                "    \n",
                "    for i in range(30):  # 30 datasets\n",
                "        y_i = y_true + np.random.randn(n_samples) * 0.5\n",
                "        \n",
                "        poly = PolynomialFeatures(degree=degree)\n",
                "        X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
                "        X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
                "        \n",
                "        theta = np.linalg.lstsq(X_poly, y_i, rcond=None)[0]\n",
                "        y_pred = X_test_poly.dot(theta)\n",
                "        predictions.append(y_pred)\n",
                "    \n",
                "    predictions = np.array(predictions)\n",
                "    mean_pred = np.mean(predictions, axis=0)\n",
                "    \n",
                "    bias_sq = np.mean((mean_pred - y_test_true)**2)\n",
                "    var = np.mean(np.var(predictions, axis=0))\n",
                "    \n",
                "    bias_list.append(bias_sq)\n",
                "    variance_list.append(var)\n",
                "    total_error_list.append(bias_sq + var)\n",
                "\n",
                "# Plot the tradeoff\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(degrees_range, bias_list, 'b-o', linewidth=2, label='Bias²', markersize=6)\n",
                "plt.plot(degrees_range, variance_list, 'r-o', linewidth=2, label='Variance', markersize=6)\n",
                "plt.plot(degrees_range, total_error_list, 'g-o', linewidth=3, label='Total Error (Bias² + Variance)', markersize=6)\n",
                "\n",
                "# Mark optimal\n",
                "optimal_idx = np.argmin(total_error_list)\n",
                "plt.axvline(x=list(degrees_range)[optimal_idx], color='gray', linestyle='--', alpha=0.7, label=f'Optimal (degree={list(degrees_range)[optimal_idx]})')\n",
                "\n",
                "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
                "plt.ylabel('Error', fontsize=12)\n",
                "plt.title('Bias-Variance Tradeoff', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Key Observations:\n",
                "\n",
                "1. **Low complexity** (degree 1-2): High bias, low variance\n",
                "   - Model is too simple\n",
                "   - Consistent but inaccurate predictions\n",
                "   \n",
                "2. **Optimal complexity** (degree ~2-3): Balanced\n",
                "   - Lowest total error\n",
                "   - Sweet spot!\n",
                "   \n",
                "3. **High complexity** (degree > 10): Low bias, high variance\n",
                "   - Model is too complex\n",
                "   - Fits noise, predictions vary wildly"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Experiment 4: Effect of Regularization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# High degree polynomial with different regularization\n",
                "degree = 15\n",
                "lambdas = [0, 0.01, 0.1, 1.0, 10.0]\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, lam in enumerate(lambdas):\n",
                "    predictions = []\n",
                "    \n",
                "    for i in range(30):\n",
                "        y_i = y_true + np.random.randn(n_samples) * 0.5\n",
                "        \n",
                "        poly = PolynomialFeatures(degree=degree)\n",
                "        X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
                "        X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
                "        \n",
                "        # Ridge regression\n",
                "        model = Ridge(alpha=lam, fit_intercept=False)\n",
                "        model.fit(X_poly, y_i)\n",
                "        y_pred = model.predict(X_test_poly)\n",
                "        predictions.append(y_pred)\n",
                "    \n",
                "    predictions = np.array(predictions)\n",
                "    mean_pred = np.mean(predictions, axis=0)\n",
                "    \n",
                "    bias_sq = np.mean((mean_pred - y_test_true)**2)\n",
                "    var = np.mean(np.var(predictions, axis=0))\n",
                "    \n",
                "    # Plot\n",
                "    for pred in predictions[:15]:\n",
                "        axes[idx].plot(X_test, pred, 'b-', alpha=0.15)\n",
                "    axes[idx].plot(X_test, y_test_true, 'r-', linewidth=3, label='True')\n",
                "    axes[idx].plot(X_test, mean_pred, 'g-', linewidth=3, label='Mean')\n",
                "    axes[idx].set_xlabel('x', fontsize=11)\n",
                "    axes[idx].set_ylabel('y', fontsize=11)\n",
                "    axes[idx].set_title(f'λ={lam} | Bias²={bias_sq:.3f}, Var={var:.3f}', fontsize=12, fontweight='bold')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "    axes[idx].set_ylim(-2, 10)\n",
                "\n",
                "# Hide extra subplot\n",
                "axes[5].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Regularization Effect:\n",
                "\n",
                "- **λ = 0**: High variance (overfits)\n",
                "- **λ = 0.01-0.1**: Reduced variance\n",
                "- **λ = 1.0**: Balanced\n",
                "- **λ = 10.0**: Increased bias (underfits)\n",
                "\n",
                "**Regularization trades variance for bias!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Bias:\n",
                "- Error from wrong assumptions\n",
                "- High bias → underfitting\n",
                "- Model too simple\n",
                "\n",
                "### Variance:\n",
                "- Error from sensitivity to training data\n",
                "- High variance → overfitting  \n",
                "- Model too complex\n",
                "\n",
                "### Total Error:\n",
                "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
                "\n",
                "### Managing the Tradeoff:\n",
                "1. **Increase model complexity** → ↓ bias, ↑ variance\n",
                "2. **Decrease model complexity** → ↑ bias, ↓ variance\n",
                "3. **Add regularization** → ↑ bias, ↓ variance\n",
                "4. **More training data** → ↓ variance (doesn't change bias much)\n",
                "5. **Feature engineering** → Can reduce both\n",
                "\n",
                "### In Practice:\n",
                "- Use **cross-validation** to find optimal complexity\n",
                "- Monitor train vs validation error\n",
                "- Start simple, add complexity if needed\n",
                "- Regularize complex models\n",
                "\n",
                "**Interview Tip**: \"The bias-variance tradeoff is fundamental: simple models have high bias but low variance; complex models have low bias but high variance. I'd use cross-validation to find the sweet spot that minimizes total error.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}