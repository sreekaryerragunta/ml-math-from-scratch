{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Mathematical Derivations\n",
    "\n",
    "This notebook contains **complete mathematical derivations** for linear regression.\n",
    "\n",
    "By the end, you'll understand:\n",
    "1. How to derive the cost function\n",
    "2. How to compute gradients (partial derivatives)\n",
    "3. How to derive the Normal Equation (closed-form solution)\n",
    "4. How gradient descent updates parameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Notation:**\n",
    "- $m$ = number of training examples\n",
    "- $n$ = number of features\n",
    "- $x^{(i)}$ = input features of $i$-th training example\n",
    "- $y^{(i)}$ = output value of $i$-th training example\n",
    "- $h_\\theta(x)$ = hypothesis function (our prediction)\n",
    "- $\\theta$ = parameters (weights)\n",
    "\n",
    "**Hypothesis (Prediction Function):**\n",
    "\n",
    "For a single feature:\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "For multiple features (vectorized):\n",
    "$$h_\\theta(x) = \\theta^T x = \\sum_{j=0}^{n} \\theta_j x_j$$\n",
    "\n",
    "Where $x_0 = 1$ (bias term).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deriving the Cost Function\n",
    "\n",
    "### Goal: Measure how \"wrong\" our predictions are\n",
    "\n",
    "For a single example, the error is:\n",
    "$$\\text{error} = h_\\theta(x^{(i)}) - y^{(i)}$$\n",
    "\n",
    "We **square** it to:\n",
    "1. Make it always positive (no cancellation)\n",
    "2. Penalize large errors more\n",
    "3. Make it differentiable everywhere\n",
    "\n",
    "$$\\text{squared error} = (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "For all $m$ examples, we take the **average**:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The $\\frac{1}{2}$ is for convenience (cancels when we take derivatives).\n",
    "\n",
    "---\n",
    "\n",
    "### Vectorized Form\n",
    "\n",
    "Let:\n",
    "- $X$ = design matrix of shape $(m \\times (n+1))$ where each row is $x^{(i)}$\n",
    "- $y$ = output vector of shape $(m \\times 1)$\n",
    "- $\\theta$ = parameter vector of shape $((n+1) \\times 1)$\n",
    "\n",
    "Then:\n",
    "$$h_\\theta(X) = X\\theta$$\n",
    "\n",
    "And the cost function becomes:\n",
    "$$J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent - Computing Partial Derivatives\n",
    "\n",
    "### What is a Gradient?\n",
    "\n",
    "The **gradient** $\\nabla J(\\theta)$ is a vector of partial derivatives:\n",
    "\n",
    "$$\\nabla J(\\theta) = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{bmatrix}$$\n",
    "\n",
    "It points in the direction of **steepest increase** in $J$.\n",
    "\n",
    "To minimize $J$, we move in the **opposite** direction: $-\\nabla J(\\theta)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Deriving $\\frac{\\partial J}{\\partial \\theta_j}$\n",
    "\n",
    "Start with the cost function:\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Recall: $h_\\theta(x) = \\sum_{j=0}^{n} \\theta_j x_j$\n",
    "\n",
    "**Step 1:** Apply chain rule\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\right]$$\n",
    "\n",
    "**Step 2:** Move the constant and sum outside\n",
    "\n",
    "$$= \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Step 3:** Apply chain rule to $(...)^2$\n",
    "\n",
    "Let $u = h_\\theta(x^{(i)}) - y^{(i)}$, then $\\frac{\\partial u^2}{\\partial \\theta_j} = 2u \\frac{\\partial u}{\\partial \\theta_j}$\n",
    "\n",
    "$$= \\frac{1}{2m} \\sum_{i=1}^{m} 2(h_\\theta(x^{(i)}) - y^{(i)}) \\cdot \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j}$$\n",
    "\n",
    "**Step 4:** The 2 cancels with $\\frac{1}{2}$\n",
    "\n",
    "$$= \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j}$$\n",
    "\n",
    "**Step 5:** Compute $\\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j}$\n",
    "\n",
    "Since $h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... + \\theta_n x_n$\n",
    "\n",
    "$$\\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j} = x_j^{(i)}$$\n",
    "\n",
    "**Final Result:**\n",
    "\n",
    "$$\\boxed{\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Vectorized Gradient\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\\boxed{\\nabla J(\\theta) = \\frac{1}{m} X^T (X\\theta - y)}$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the design matrix $(m \\times (n+1))$\n",
    "- $X\\theta$ is the predictions $(m \\times 1)$\n",
    "- $X\\theta - y$ is the error vector $(m \\times 1)$\n",
    "- $X^T$ multiplies errors by features, giving gradient $((n+1) \\times 1)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Update Rule\n",
    "\n",
    "Now that we have the gradient, we can update parameters:\n",
    "\n",
    "**For each parameter:**\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "$$\\boxed{\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}}$$\n",
    "\n",
    "**Vectorized:**\n",
    "\n",
    "$$\\boxed{\\theta := \\theta - \\alpha \\frac{1}{m} X^T (X\\theta - y)}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- We repeat this update until convergence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normal Equation - Closed-Form Solution\n",
    "\n",
    "### Can we solve for $\\theta$ directly without iteration?\n",
    "\n",
    "Yes! By setting the gradient to zero.\n",
    "\n",
    "**Start with the vectorized cost:**\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)$$\n",
    "\n",
    "**Expand:**\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} (\\theta^T X^T - y^T)(X\\theta - y)$$\n",
    "\n",
    "$$= \\frac{1}{2m} (\\theta^T X^T X \\theta - \\theta^T X^T y - y^T X \\theta + y^T y)$$\n",
    "\n",
    "Since $\\theta^T X^T y$ is a scalar, it equals its transpose: $\\theta^T X^T y = y^T X \\theta$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} (\\theta^T X^T X \\theta - 2y^T X \\theta + y^T y)$$\n",
    "\n",
    "**Take the gradient with respect to $\\theta$:**\n",
    "\n",
    "Using matrix calculus rules:\n",
    "- $\\frac{\\partial}{\\partial \\theta}(\\theta^T A \\theta) = 2A\\theta$ (if $A$ is symmetric)\n",
    "- $\\frac{\\partial}{\\partial \\theta}(b^T \\theta) = b$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\frac{1}{2m}(2 X^T X \\theta - 2 X^T y)$$\n",
    "\n",
    "$$= \\frac{1}{m}(X^T X \\theta - X^T y)$$\n",
    "\n",
    "**Set gradient to zero:**\n",
    "\n",
    "$$\\frac{1}{m}(X^T X \\theta - X^T y) = 0$$\n",
    "\n",
    "$$X^T X \\theta = X^T y$$\n",
    "\n",
    "**Solve for $\\theta$:**\n",
    "\n",
    "$$\\boxed{\\theta = (X^T X)^{-1} X^T y}$$\n",
    "\n",
    "This is the **Normal Equation**.\n",
    "\n",
    "---\n",
    "\n",
    "### When does this work?\n",
    "\n",
    "- Requires $(X^T X)$ to be **invertible** (non-singular)\n",
    "- Fails when:\n",
    "  - Features are linearly dependent (multicollinearity)\n",
    "  - More features than examples $(n > m)$\n",
    "- Computationally expensive: $O(n^3)$ for matrix inversion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary: The Two Approaches\n",
    "\n",
    "### Gradient Descent (Iterative)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize $\\theta$ randomly\n",
    "2. Repeat until convergence:\n",
    "   $$\\theta := \\theta - \\alpha \\frac{1}{m} X^T (X\\theta - y)$$\n",
    "\n",
    "**Pros:**\n",
    "- Works with any dataset size\n",
    "- Scales to large $n$\n",
    "- Generalizes to other algorithms (logistic regression, neural networks)\n",
    "\n",
    "**Cons:**\n",
    "- Need to choose $\\alpha$\n",
    "- Requires iterations\n",
    "- Need to check convergence\n",
    "\n",
    "---\n",
    "\n",
    "### Normal Equation (Closed-Form)\n",
    "\n",
    "**Algorithm:**\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "**Pros:**\n",
    "- Exact answer in one step\n",
    "- No hyperparameters\n",
    "- No iteration needed\n",
    "\n",
    "**Cons:**\n",
    "- Slow for large $n$ (matrix inversion is $O(n^3)$)\n",
    "- Requires $(X^T X)$ to be invertible\n",
    "- Doesn't generalize to other algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### When to use which?\n",
    "\n",
    "| Condition | Use |\n",
    "|-----------|-----|\n",
    "| $n \\leq 1000$ | Normal Equation |\n",
    "| $n > 10000$ | Gradient Descent |\n",
    "| Features are linearly dependent | Gradient Descent (or add regularization) |\n",
    "| Want to learn gradient descent | Gradient Descent (for understanding) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Cost Function**: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "2. **Gradient**: $\\nabla J(\\theta) = \\frac{1}{m} X^T (X\\theta - y)$\n",
    "\n",
    "3. **Gradient Descent Update**: $\\theta := \\theta - \\alpha \\nabla J(\\theta)$\n",
    "\n",
    "4. **Normal Equation**: $\\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "5. The math is **simpler than it looks**:\n",
    "   - Cost = average squared error\n",
    "   - Gradient = average (error Ã— feature)\n",
    "   - Update = move opposite to gradient\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Implement these equations in Python in `linear_regression_from_scratch.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
